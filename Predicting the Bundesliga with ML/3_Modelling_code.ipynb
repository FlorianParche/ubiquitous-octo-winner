{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains the Feature Engineering, Hyperparameter tuning and Modelling steps.\n",
    "\n",
    "#### Notebook 1: Extract-Transform-Load\n",
    "#### Notebook 2: Data Visualization\n",
    "#### Notebook 3: Feature Engineering, Hyperparameter tuning and Modelling\n",
    "#### Notebook 4: Result Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time as time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import np_utils\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LeakyReLU\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import - from ETL notebook\n",
    "Dataset = pd.read_csv(r\"C:\\Users\\### LOCAL PATH ###\\dataforcapstone.txt\", sep = \"\\t\")\n",
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Teams_list = list(Dataset[\"HomeTeam\"].drop_duplicates())\n",
    "Seasons_list = list(Dataset[\"Season start\"].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding the previous season's summary statistics as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Season_data = pd.DataFrame()\n",
    "\n",
    "for Team in Teams_list:\n",
    "    data1 = Dataset.loc[Dataset[\"HomeTeam\"] == Team]\n",
    "    data2 = Dataset.loc[Dataset[\"AwayTeam\"] == Team]\n",
    "    \n",
    "    for year in Seasons_list:\n",
    "        \n",
    "        data1_b = data1.loc[data1[\"Season start\"] == year]\n",
    "        data2_b = data2.loc[data2[\"Season start\"] == year]\n",
    "\n",
    "        Goals_H_for = data1_b[\"Home Goals\"].sum()\n",
    "        Goals_H_against = data1_b[\"Away Goals\"].sum()\n",
    "        Goals_A_for = data2_b[\"Away Goals\"].sum()\n",
    "        Goals_A_against = data2_b[\"Home Goals\"].sum()\n",
    "\n",
    "        # \"Counting\" Wins, draws, losses - first at home then away\n",
    "        Wins_H = data1_b[\"Result\"].str.count(\"H\").sum()\n",
    "        Ties_H = data1_b[\"Result\"].str.count(\"D\").sum()\n",
    "        Losses_H = data1_b[\"Result\"].str.count(\"A\").sum()\n",
    "        \n",
    "        Wins_A = data2_b[\"Result\"].str.count(\"A\").sum()\n",
    "        Ties_A = data2_b[\"Result\"].str.count(\"D\").sum()\n",
    "        Losses_A = data2_b[\"Result\"].str.count(\"H\").sum()\n",
    "        \n",
    "        Total_points = Wins_H * 3 + Ties_H + Wins_A * 3 + Ties_A\n",
    "              \n",
    "        Season_data = Season_data.append([[Team, year, Total_points, Wins_H, Ties_H, Losses_H, Goals_H_for, Goals_H_against, Wins_A, Ties_A, Losses_A, Goals_A_for, Goals_A_against]])\n",
    "\n",
    "Season_data.columns = [\"Team\",\"Season start\",\"Total Points\",\"Home W\",\"Home T\",\"Home L\",\"Home Goals For\", \"Home Goals Against\",\"Away W\",\"Away T\",\"Away L\",\"Away Goals For\", \"Away Goals Against\"]\n",
    "Season_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combining the data - prev season summary for each Game\n",
    "Season_data[\"prev season\"] = pd.to_numeric(Season_data[\"Season start\"]) +1\n",
    "Season_data = Season_data.drop(\"Season start\", axis=1)\n",
    "# To merge\n",
    "Season_data2 = Season_data.rename(columns={\"Team\":\"AwayTeam\",\"Total Points\":\"T2 Total Points\",\"Home W\":\"T2 Home W\",\"Home T\":\"T2 Home T\",\"Home L\":\"T2 Home L\", \"Home Goals For\":\"T2 Home Goals For\",\"Home Goals Against\":\"T2 Home Goals Against\",\"Away W\":\"T2 Away W\",\"Away T\":\"T2 Away T\",\"Away L\":\"T2 Away L\",\"Away Goals For\":\"T2 Away Goals For\",\"Away Goals Against\":\"T2 Away Goals Against\"})\n",
    "Season_data = Season_data.rename(columns={\"Team\":\"HomeTeam\",\"Total Points\":\"T1 Total Points\",\"Home W\":\"T1 Home W\",\"Home T\":\"T1 Home T\",\"Home L\":\"T1 Home L\", \"Home Goals For\":\"T1 Home Goals For\",\"Home Goals Against\":\"T1 Home Goals Against\",\"Away W\":\"T1 Away W\",\"Away T\":\"T1 Away T\",\"Away L\":\"T1 Away L\",\"Away Goals For\":\"T1 Away Goals For\",\"Away Goals Against\":\"T1 Away Goals Against\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dataset[\"Season start\"] = pd.to_numeric(Dataset[\"Season start\"])\n",
    "Dataset[\"prev season\"] = pd.to_numeric(Dataset[\"Season start\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a test set to avoid overfitting when tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impose randomly set Train-Val-Test sets\n",
    "for i in range(Dataset.shape[0]):\n",
    "    Dataset.loc[i,\"Test set identifier\"] = np.random.randn()\n",
    "    Dataset.loc[i,\"Val set identifier\"] = np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merging season data\n",
    "Dataset = Dataset.merge(Season_data, how=\"left\",on=[\"prev season\",\"HomeTeam\"])\n",
    "Dataset = Dataset.merge(Season_data2, how=\"left\", on=[\"prev season\",\"AwayTeam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### The weight for the previous season's summary statistics defined as a hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prev_season_weight(weight, Dataset):\n",
    "    \n",
    "    Dataset[\"Weight\"] = (Dataset[\"Gameday\"] ** (weight/100) ) / Dataset[\"Gameday\"]\n",
    "    \n",
    "    for i in range(13,Dataset.shape[1]):\n",
    "        Dataset.iloc[:,i] = np.where(Dataset.iloc[:, i].notnull(),np.multiply(Dataset.iloc[:, i] , Dataset[\"Weight\"]),np.nan)\n",
    "    \n",
    "    Dataset2 = Dataset.drop([\"Weight\"], axis = 1)\n",
    "    Indicator = Dataset2[[\"Season start\",\"Gameday\"]].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return Dataset2, Indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Averaging statistics over the previous n games as additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prev_games_avg_data(n, Indicator, Dataset2):\n",
    "    Prev_Games_avg_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for i in range(Indicator.shape[0]-n+1):\n",
    "    \n",
    "        data = Indicator.iloc[i:(i+n),:].merge(Dataset2[[\"Season start\", \"Gameday\", \"HomeTeam\", \"AwayTeam\", \"Home Goals\", \"Away Goals\", \"Result\", \"Odds H\",\"Odds D\",\"Odds A\"]], how=\"left\", on=[\"Season start\",\"Gameday\"])\n",
    "    \n",
    "        for Team in Teams_list:\n",
    "            data1_b = data.loc[data[\"HomeTeam\"] == Team].reset_index(drop=True)\n",
    "            data2_b = data.loc[data[\"AwayTeam\"] == Team].reset_index(drop=True)\n",
    "    \n",
    "            Goals_H_for = data1_b[\"Home Goals\"].mean() \n",
    "            Goals_H_against = data1_b[\"Away Goals\"].mean() \n",
    "            Goals_A_for = data2_b[\"Away Goals\"].mean()\n",
    "            Goals_A_against = data2_b[\"Home Goals\"].mean()\n",
    "\n",
    "        # Avg Odds\n",
    "            H_Odds_when_H = data1_b[\"Odds H\"].mean()\n",
    "            D_Odds_when_H = data1_b[\"Odds D\"].mean() \n",
    "            A_Odds_when_H = data1_b[\"Odds A\"].mean() \n",
    "            H_Odds_when_A = data2_b[\"Odds H\"].mean() \n",
    "            D_Odds_when_A = data2_b[\"Odds D\"].mean()\n",
    "            A_Odds_when_A = data2_b[\"Odds A\"].mean()\n",
    "\n",
    "        # \"Counting\" Wins, draws, losses - first at home then away\n",
    "            if float(data1_b.shape[0]) != 0:\n",
    "                Wins_H = data1_b[\"Result\"].str.count(\"H\").sum() / float(data1_b.shape[0])\n",
    "                Ties_H = data1_b[\"Result\"].str.count(\"D\").sum() / float(data1_b.shape[0])\n",
    "                Losses_H = data1_b[\"Result\"].str.count(\"A\").sum() / float(data1_b.shape[0])\n",
    "            else:\n",
    "                Wins_H = 0\n",
    "                Ties_H = 0\n",
    "                Losses_H = 0\n",
    "\n",
    "            if float(data2_b.shape[0]) != 0:\n",
    "                Wins_A = data2_b[\"Result\"].str.count(\"A\").sum() / float(data2_b.shape[0])\n",
    "                Ties_A = data2_b[\"Result\"].str.count(\"D\").sum() / float(data2_b.shape[0]) \n",
    "                Losses_A = data2_b[\"Result\"].str.count(\"H\").sum() / float(data2_b.shape[0])\n",
    "            else:\n",
    "                Wins_A = 0\n",
    "                Ties_A = 0\n",
    "                Losses_A = 0\n",
    "                \n",
    "            Total_points = (data1_b[\"Result\"].str.count(\"H\").sum() * 3 + data1_b[\"Result\"].str.count(\"D\").sum() + data2_b[\"Result\"].str.count(\"A\").sum() * 3 + data2_b[\"Result\"].str.count(\"D\").sum()) / n # avg points\n",
    "            Total_G_For = (data1_b[\"Home Goals\"].sum()+ data2_b[\"Away Goals\"].sum()) /n\n",
    "            Total_G_Against = (data1_b[\"Away Goals\"].sum()+data2_b[\"Home Goals\"].sum()) /n\n",
    "            \n",
    "            Prev_Games_avg_data = Prev_Games_avg_data.append([[Team, data.iloc[-1,0], data.iloc[-1,1],Total_points, Total_G_For, Total_G_Against, Wins_H, Ties_H, Losses_H, Goals_H_for, Goals_H_against, Wins_A, Ties_A, Losses_A, Goals_A_for, Goals_A_against,H_Odds_when_H,D_Odds_when_H,A_Odds_when_H,H_Odds_when_A,D_Odds_when_A,A_Odds_when_A]])\n",
    "    \n",
    "    Prev_Games_avg_data.columns = [\"Team\",\"Season start\",\"Gameday\",\"Avg Total Points\",\"Avg. Total Goals For\", \"Avg. Total Goals Against\", \"Avg Home W\",\"Avg Home T\",\"Avg Home L\",\"Avg Home Goals For\", \"Avg Home Goals Against\",\"Avg Away W\",\"Avg Away T\",\"Avg Away L\",\"Avg Away Goals For\", \"Avg Away Goals Against\",\"Avg H Odds when H\",\"Avg D Odds when H\",\"Avg A Odds when H\",\"Avg H Odds when A\",\"Avg D Odds when A\",\"Avg A Odds when A\"]\n",
    "    Prev_Games_avg_data.reset_index(drop=True, inplace=True)\n",
    "    Prev_Games_avg_data = Prev_Games_avg_data.dropna(subset=[\"Avg Total Points\"])\n",
    "    \n",
    "    return Prev_Games_avg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ranking average statistics among all Teams over the previous m games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prev_games_avg_data_for_rank(m, Indicator, Dataset2):\n",
    "    Prev_Games_avg_data_for_rank = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for i in range(Indicator.shape[0]-m+1):\n",
    "    \n",
    "        data = Indicator.iloc[i:(i+m),:].merge(Dataset2[[\"Season start\", \"Gameday\", \"HomeTeam\", \"AwayTeam\", \"Home Goals\", \"Away Goals\", \"Result\", \"Odds H\",\"Odds D\",\"Odds A\"]], how=\"left\", on=[\"Season start\",\"Gameday\"])\n",
    "    \n",
    "        for Team in Teams_list:\n",
    "            data1_b = data.loc[data[\"HomeTeam\"] == Team].reset_index(drop=True)\n",
    "            data2_b = data.loc[data[\"AwayTeam\"] == Team].reset_index(drop=True)\n",
    "\n",
    "            Goals_H_for = data1_b[\"Home Goals\"].mean() \n",
    "            Goals_H_against = data1_b[\"Away Goals\"].mean()\n",
    "            Goals_A_for = data2_b[\"Away Goals\"].mean() \n",
    "            Goals_A_against = data2_b[\"Home Goals\"].mean()\n",
    "\n",
    "        # Avg Odds\n",
    "            H_Odds_when_H = data1_b[\"Odds H\"].mean()\n",
    "            D_Odds_when_H = data1_b[\"Odds D\"].mean()\n",
    "            A_Odds_when_H = data1_b[\"Odds A\"].mean()\n",
    "            H_Odds_when_A = data2_b[\"Odds H\"].mean() \n",
    "            D_Odds_when_A = data2_b[\"Odds D\"].mean() \n",
    "            A_Odds_when_A = data2_b[\"Odds A\"].mean() \n",
    "\n",
    "        # \"Counting\" Wins, draws, losses - first at home then away\n",
    "            if float(data1_b.shape[0]) != 0:\n",
    "                Wins_H = data1_b[\"Result\"].str.count(\"H\").sum() / float(data1_b.shape[0])\n",
    "                Ties_H = data1_b[\"Result\"].str.count(\"D\").sum() / float(data1_b.shape[0])\n",
    "                Losses_H = data1_b[\"Result\"].str.count(\"A\").sum()/ float(data1_b.shape[0])\n",
    "            else:\n",
    "                Wins_H = 0\n",
    "                Ties_H = 0\n",
    "                Losses_H = 0\n",
    "                \n",
    "            if float(data2_b.shape[0]) != 0:\n",
    "                Wins_A = data2_b[\"Result\"].str.count(\"A\").sum()/ float(data2_b.shape[0])\n",
    "                Ties_A = data2_b[\"Result\"].str.count(\"D\").sum() / float(data2_b.shape[0])  \n",
    "                Losses_A = data2_b[\"Result\"].str.count(\"H\").sum() / float(data2_b.shape[0])\n",
    "            else:\n",
    "                Wins_A = 0\n",
    "                Ties_A = 0\n",
    "                Losses_A = 0\n",
    "                \n",
    "            Total_points = (data1_b[\"Result\"].str.count(\"H\").sum() * 3 + data1_b[\"Result\"].str.count(\"D\").sum() + data2_b[\"Result\"].str.count(\"A\").sum() * 3 + data2_b[\"Result\"].str.count(\"D\").sum()) / m # avg points\n",
    "            Total_G_For = (data1_b[\"Home Goals\"].sum() + data2_b[\"Away Goals\"].sum()) /m\n",
    "            Total_G_Against = (data1_b[\"Away Goals\"].sum()+data2_b[\"Home Goals\"].sum())/m\n",
    "            \n",
    "            Prev_Games_avg_data_for_rank = Prev_Games_avg_data_for_rank.append([[Team, data.iloc[-1,0], data.iloc[-1,1],Total_points, Total_G_For, Total_G_Against, Wins_H, Ties_H, Losses_H, Goals_H_for, Goals_H_against, Wins_A, Ties_A, Losses_A, Goals_A_for, Goals_A_against,H_Odds_when_H,D_Odds_when_H,A_Odds_when_H,H_Odds_when_A,D_Odds_when_A,A_Odds_when_A]])\n",
    "    \n",
    "    Prev_Games_avg_data_for_rank.columns = [\"Team\",\"Season start\",\"Gameday\",\"Avg Total Points\",\"Avg. Total Goals For\", \"Avg. Total Goals Against\", \"Avg Home W\",\"Avg Home T\",\"Avg Home L\",\"Avg Home Goals For\", \"Avg Home Goals Against\",\"Avg Away W\",\"Avg Away T\",\"Avg Away L\",\"Avg Away Goals For\", \"Avg Away Goals Against\",\"Avg H Odds when H\",\"Avg D Odds when H\",\"Avg A Odds when H\",\"Avg H Odds when A\",\"Avg D Odds when A\",\"Avg A Odds when A\"]\n",
    "    Prev_Games_avg_data_for_rank.reset_index(drop=True, inplace=True)\n",
    "    Prev_Games_avg_data_for_rank = Prev_Games_avg_data_for_rank.dropna(subset=[\"Avg Total Points\"])\n",
    "    Indicator2 = Prev_Games_avg_data_for_rank[[\"Season start\",\"Gameday\"]].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return Prev_Games_avg_data_for_rank, Indicator2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prev_games_ranking(Indicator2, Prev_Games_avg_data_for_rank):\n",
    "    Prev_Games_ranking = pd.DataFrame()\n",
    "\n",
    "    for i in range(Indicator2.shape[0]):\n",
    "\n",
    "        data = Prev_Games_avg_data_for_rank.loc[(Prev_Games_avg_data_for_rank[\"Season start\"] == Indicator2.iloc[i,0])&(Prev_Games_avg_data_for_rank[\"Gameday\"] == Indicator2.iloc[i,1])]\n",
    "        Rankings = data.drop([\"Team\",\"Season start\",\"Gameday\"], axis=1).rank(axis = 0, ascending = False)\n",
    "    \n",
    "        Prev_Games_ranking = Prev_Games_ranking.append(Rankings)\n",
    "    \n",
    "    Prev_Games_ranking.columns = [\"Rank \" + str(col) for col in Prev_Games_ranking.columns]\n",
    "    \n",
    "    return Prev_Games_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_merge(Prev_Games_avg_data, Prev_Games_avg_data_for_rank, Prev_Games_ranking, Dataset2):\n",
    "    Data_from_prev_games_1 = Prev_Games_avg_data\n",
    "    Data_from_prev_games_2 = pd.concat([Prev_Games_avg_data_for_rank[[\"Team\",\"Season start\",\"Gameday\"]], Prev_Games_ranking], axis=1)\n",
    "    \n",
    "    # prep for merge\n",
    "    Data_from_prev_games_1[\"Season start\"] = pd.to_numeric(Data_from_prev_games_1[\"Season start\"])\n",
    "    Data_from_prev_games_1[\"Gameday\"] = pd.to_numeric(Data_from_prev_games_1[\"Gameday\"])\n",
    "\n",
    "    Data_from_prev_games_1[\"Season start\"] = np.where(Data_from_prev_games_1[\"Gameday\"] == 34, Data_from_prev_games_1[\"Season start\"] +1,Data_from_prev_games_1[\"Season start\"])\n",
    "    Data_from_prev_games_1[\"Gameday\"] = np.where(Data_from_prev_games_1[\"Gameday\"] == 34, 1,Data_from_prev_games_1[\"Gameday\"]+1)\n",
    "\n",
    "#\n",
    "    Data_from_prev_games_2[\"Season start\"] = pd.to_numeric(Data_from_prev_games_2[\"Season start\"])\n",
    "    Data_from_prev_games_2[\"Gameday\"] = pd.to_numeric(Data_from_prev_games_2[\"Gameday\"])\n",
    "\n",
    "    Data_from_prev_games_2[\"Season start\"] = np.where(Data_from_prev_games_2[\"Gameday\"] == 34, Data_from_prev_games_2[\"Season start\"] +1,Data_from_prev_games_2[\"Season start\"])\n",
    "    Data_from_prev_games_2[\"Gameday\"] = np.where(Data_from_prev_games_2[\"Gameday\"] == 34, 1,Data_from_prev_games_2[\"Gameday\"]+1)\n",
    "    \n",
    "    #\n",
    "    x1 = Data_from_prev_games_1[[\"Team\", \"Season start\", \"Gameday\", \"Avg Total Points\",\"Avg. Total Goals For\",\"Avg. Total Goals Against\",\"Avg Home W\", \"Avg Home T\", \"Avg Home L\", \"Avg Home Goals For\",\"Avg Home Goals Against\"]]\n",
    "    x1.columns = [\"Team 1 \" + str(col) for col in x1.columns]\n",
    "    x1 = x1.rename(columns={\"Team 1 Team\":\"HomeTeam\",\"Team 1 Season start\":\"Season start\",\"Team 1 Gameday\":\"Gameday\"})\n",
    "\n",
    "    x2 = Data_from_prev_games_1[[\"Team\", \"Season start\", \"Gameday\", \"Avg Total Points\",\"Avg. Total Goals For\",\"Avg. Total Goals Against\",\"Avg Away W\", \"Avg Away T\", \"Avg Away L\", \"Avg Away Goals For\",\"Avg Away Goals Against\"]]\n",
    "    x2.columns = [\"Team 2 \" + str(col) for col in x2.columns]\n",
    "    x2 = x2.rename(columns={\"Team 2 Team\":\"AwayTeam\",\"Team 2 Season start\":\"Season start\",\"Team 2 Gameday\":\"Gameday\"})\n",
    "\n",
    "#\n",
    "    x3 = Data_from_prev_games_2[[\"Team\", \"Season start\", \"Gameday\",\"Rank Avg Total Points\",\"Rank Avg. Total Goals For\",\"Rank Avg. Total Goals Against\",\"Rank Avg Home W\",\"Rank Avg Home T\", \"Rank Avg Home L\", \"Rank Avg Home Goals For\",\"Rank Avg Home Goals Against\"]]\n",
    "    x3.columns = [\"Team 1 \" + str(col) for col in x3.columns]\n",
    "    x3 = x3.rename(columns={\"Team 1 Team\":\"HomeTeam\",\"Team 1 Season start\":\"Season start\",\"Team 1 Gameday\":\"Gameday\"})\n",
    "\n",
    "    x4 = Data_from_prev_games_2[[\"Team\", \"Season start\", \"Gameday\",\"Rank Avg Total Points\",\"Rank Avg. Total Goals For\",\"Rank Avg. Total Goals Against\",\"Rank Avg Away W\",\"Rank Avg Away T\", \"Rank Avg Away L\", \"Rank Avg Away Goals For\",\"Rank Avg Away Goals Against\"]]\n",
    "    x4.columns = [\"Team 2 \" + str(col) for col in x4.columns]\n",
    "    x4 = x4.rename(columns={\"Team 2 Team\":\"AwayTeam\",\"Team 2 Season start\":\"Season start\",\"Team 2 Gameday\":\"Gameday\"})\n",
    "    \n",
    "    Dataset2[\"Season start\"] = pd.to_numeric(Dataset2[\"Season start\"])\n",
    "    Dataset2[\"Gameday\"] = pd.to_numeric(Dataset2[\"Gameday\"])\n",
    "\n",
    "    Data = Dataset2.merge(x1, how=\"left\",on=[\"HomeTeam\",\"Season start\",\"Gameday\"])\n",
    "    Data = Data.merge(x2, how=\"left\",on=[\"AwayTeam\",\"Season start\",\"Gameday\"])\n",
    "    Data = Data.merge(x3, how=\"left\",on=[\"HomeTeam\",\"Season start\",\"Gameday\"])\n",
    "    Data = Data.merge(x4, how=\"left\",on=[\"AwayTeam\",\"Season start\",\"Gameday\"])\n",
    "    \n",
    "    # getting rid of some irrelevant data\n",
    "    Data = Data.drop([\"T1 Away W\",\"T1 Away T\",\"T1 Away L\",\"T1 Away Goals For\",\"T1 Away Goals Against\",\"T2 Home W\",\"T2 Home T\",\"T2 Home L\",\"T2 Home Goals For\",\"T2 Home Goals Against\",\"prev season\"], axis=1)\n",
    "    Data = Data.dropna(subset=[\"T1 Total Points\"])\n",
    "    Data = Data.loc[Data[\"T1 Total Points\"] != 0]\n",
    "    Data = Data.dropna(subset=[\"T2 Total Points\"])\n",
    "    Data = Data.loc[Data[\"T2 Total Points\"] != 0]\n",
    "    Data = Data.reset_index(drop=True)\n",
    "    \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choice of which type of Scaler as a hyperparamter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scaleData(data):\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scaleData2(data):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hyperparameters:\n",
    "# g - type of scaler (MinMax / Standard / [-1,1] - except for dummies)\n",
    "# k - max depth of the tree\n",
    "\n",
    "def Tree_Modelling(Data, g, k):\n",
    "    \n",
    "    try:\n",
    "        Data = Data.drop([\"Weight\"], axis=1).dropna()\n",
    "    except:\n",
    "        Data = Data.dropna()\n",
    "    try:\n",
    "        Data = Data.drop([\"prev season\"], axis=1).dropna()\n",
    "    except:\n",
    "        Data = Data.dropna()\n",
    "    Data = Data.dropna()\n",
    "    \n",
    "\n",
    "    Data = Data.sort_values([\"Test set identifier\"], ascending = True)\n",
    "    \n",
    "    \n",
    "    Test_set_data = Data.iloc[:(round(0.15*Data.shape[0])),:]\n",
    "    Train_and_Val_set = Data.iloc[(round(0.15*Data.shape[0])):,:]\n",
    "    Train_and_Val_set = Train_and_Val_set.drop([\"Test set identifier\"], axis = 1)\n",
    "    \n",
    "    Train_and_Val_set = Train_and_Val_set.sort_values([\"Val set identifier\"], ascending = True)\n",
    "    Val_set = Train_and_Val_set.iloc[:(round(0.12*Train_and_Val_set.shape[0])),:]\n",
    "    Train_set = Train_and_Val_set.iloc[(round(0.12*Train_and_Val_set.shape[0])):,:]\n",
    "    Train_set = Train_set.drop([\"Val set identifier\"], axis = 1)\n",
    "    Val_set = Val_set.drop([\"Val set identifier\"], axis = 1)\n",
    "    \n",
    "    Target = Train_set.loc[:, \"Result\"]\n",
    "    Val_Target = Val_set.loc[:,\"Result\"]\n",
    "    Features = Train_set.iloc[:, 7:].values\n",
    "    Val_Features = Val_set.iloc[:, 7:].values\n",
    "    \n",
    "    \n",
    "    if g == 0:\n",
    "        scaled_Features = scaleData(Features)\n",
    "        scaled_Val = scaleData(Val_Features)\n",
    "    elif g == 1:\n",
    "        scaled_Features = scaleData2(Features)\n",
    "        scaled_Val = scaleData2(Val_Features)\n",
    "    else:\n",
    "        \n",
    "        scaled_Features = Features\n",
    "        scaled_Val = Val_Features\n",
    "    \n",
    "    train_set = scaled_Features\n",
    "    val_set = scaled_Val\n",
    "    train_label = Target\n",
    "    val_label = Val_Target\n",
    "    \n",
    "    Result_Tree = DecisionTreeClassifier(criterion='entropy', max_depth = k)  \n",
    "    Result_Tree.fit(train_set, train_label)\n",
    "    \n",
    "    \n",
    "    score_train = metrics.accuracy_score(train_label, Result_Tree.predict(train_set))\n",
    "    \n",
    "    score_test = metrics.accuracy_score(val_label, Result_Tree.predict(val_set))\n",
    "    \n",
    "    \n",
    "    return score_train, score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support-Vector-Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hyperparameters:\n",
    "# g - type of scaler (MinMax / Standard / [-1,1] - except for dummies)\n",
    "# k - type of kernel (linear / polynomial / gaussian)\n",
    "# p - polynomial degree\n",
    "\n",
    "def SVM_Modelling(Data, g, k, p):\n",
    "    \n",
    "    try:\n",
    "        Data = Data.drop([\"Weight\"], axis=1).dropna()\n",
    "    except:\n",
    "        Data = Data.dropna()\n",
    "    try:\n",
    "        Data = Data.drop([\"prev season\"], axis=1).dropna()\n",
    "    except:\n",
    "        Data = Data.dropna()\n",
    "    Data = Data.dropna()\n",
    "    \n",
    "    Data = Data.sort_values([\"Test set identifier\"], ascending = True)\n",
    "    \n",
    "    Test_set_data = Data.iloc[:(round(0.15*Data.shape[0])),:]\n",
    "    Train_and_Val_set = Data.iloc[(round(0.15*Data.shape[0])):,:]\n",
    "    Train_and_Val_set = Train_and_Val_set.drop([\"Test set identifier\"], axis = 1)\n",
    "    \n",
    "    Train_and_Val_set = Train_and_Val_set.sort_values([\"Val set identifier\"], ascending = True)\n",
    "    Val_set = Train_and_Val_set.iloc[:(round(0.12*Train_and_Val_set.shape[0])),:]\n",
    "    Train_set = Train_and_Val_set.iloc[(round(0.12*Train_and_Val_set.shape[0])):,:]\n",
    "    Train_set = Train_set.drop([\"Val set identifier\"], axis = 1)\n",
    "    Val_set = Val_set.drop([\"Val set identifier\"], axis = 1)\n",
    "    \n",
    "    Target = Train_set.loc[:, \"Result\"]\n",
    "    Val_Target = Val_set.loc[:,\"Result\"]\n",
    "    Features = Train_set.iloc[:, 7:].values\n",
    "    Val_Features = Val_set.iloc[:, 7:].values\n",
    "    \n",
    "    \n",
    "    if g == 0:\n",
    "        scaled_Features = scaleData(Features)\n",
    "        scaled_Val = scaleData(Val_Features)\n",
    "    elif g == 1:\n",
    "        scaled_Features = scaleData2(Features)\n",
    "        scaled_Val = scaleData2(Val_Features)\n",
    "    else:\n",
    "        for i in range(Features.shape[0]):\n",
    "            for j in range(Features.shape[1]):\n",
    "                Features[i,j] = 2 * (Features[i,j] - Features[:,j].min()) / (Features[:,j].max() - Features[:,j].min())-1\n",
    "        for i in range(Val_Features.shape[0]):\n",
    "            for j in range(Val_Features.shape[1]):\n",
    "                Val_Features[i,j] = 2 * (Val_Features[i,j] - Val_Features[:,j].min()) / (Val_Features[:,j].max() - Val_Features[:,j].min())-1\n",
    "        scaled_Features = Features\n",
    "        scaled_Val = Val_Features\n",
    "    \n",
    "    train_set = scaled_Features\n",
    "    val_set = scaled_Val\n",
    "    train_label = Target\n",
    "    val_label = Val_Target\n",
    "    \n",
    "    \n",
    "    if k == 0:\n",
    "        clf = svm.SVC(kernel='linear',gamma='auto')\n",
    "    elif k == 1:\n",
    "        clf= svm.SVC(kernel ='poly', degree= p, gamma='auto')\n",
    "    else:\n",
    "        clf = svm.SVC(kernel='rbf', gamma = 'auto')\n",
    "    \n",
    "    \n",
    "    clf.fit(train_set, train_label)\n",
    "    \n",
    "    score_train = metrics.accuracy_score(train_label, clf.predict(train_set))\n",
    "    score_test = metrics.accuracy_score(val_label, clf.predict(val_set))\n",
    "    \n",
    "    \n",
    "    return score_train, score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dense - Dropout - Dense - Dropout - Dense - Dense(softmax)\n",
    "\n",
    "## Hyperparameters:\n",
    "# a - 1. Dropoutlayer dropout rate\n",
    "# b - 2. Dropoutlayer dropout rate\n",
    "# c - 2. Dense layer number of nodes\n",
    "# d - 3. Dense layer number of nodes\n",
    "# e - Number of epochs - fixed to 40\n",
    "# f - Batch size\n",
    "# g - type of scaler (MinMax / Standard / [-1,1] - except for dummies)\n",
    "# h - type of optimizer (Adam / Adadelta / Adagrad / Stochastic Gradient Descent)\n",
    "# z - type of activation function (tanh / relu)\n",
    "\n",
    "def NN_Modelling(Data, a, b, c, d, e, f, g, h,z):\n",
    "    \n",
    "    try:\n",
    "        Data = Data.drop([\"Weight\"], axis=1).dropna()\n",
    "    except:\n",
    "        Data = Data.dropna()\n",
    "    try:\n",
    "        Data = Data.drop([\"prev season\"], axis=1).dropna()\n",
    "    except:\n",
    "        Data = Data.dropna()\n",
    "    \n",
    "    Data = Data.sort_values([\"Test set identifier\"], ascending = True)\n",
    "\n",
    "    Test_set_data = Data.iloc[:(round(0.15*Data.shape[0])),:]\n",
    "    Train_and_Val_set = Data.iloc[(round(0.15*Data.shape[0])):,:]\n",
    "    Train_and_Val_set = Train_and_Val_set.drop([\"Test set identifier\"], axis = 1)\n",
    "    \n",
    "    Train_and_Val_set = Train_and_Val_set.sort_values([\"Val set identifier\"], ascending = True)\n",
    "    Val_set = Train_and_Val_set.iloc[:(round(0.12*Train_and_Val_set.shape[0])),:]\n",
    "    Train_set = Train_and_Val_set.iloc[(round(0.12*Train_and_Val_set.shape[0])):,:]\n",
    "    Train_set = Train_set.drop([\"Val set identifier\"], axis = 1)\n",
    "    Val_set = Val_set.drop([\"Val set identifier\"], axis = 1)\n",
    "    \n",
    "    Target = Train_set.loc[:, \"Result\"]\n",
    "    Val_Target = Val_set.loc[:,\"Result\"]\n",
    "    Features = Train_set.iloc[:, 7:].values\n",
    "    Val_Features = Val_set.iloc[:, 7:].values    \n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Target)\n",
    "    encoded_Y = encoder.transform(Target)\n",
    "    dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "    encoded_val = encoder.transform(Val_Target)\n",
    "    dummy_val = np_utils.to_categorical(encoded_val)\n",
    "    \n",
    "    if g == 0:\n",
    "        scaled_Features = scaleData(Features)\n",
    "        scaled_Val = scaleData(Val_Features)\n",
    "    elif g == 1:\n",
    "        scaled_Features = scaleData2(Features)\n",
    "        scaled_Val = scaleData2(Val_Features)\n",
    "    else:\n",
    "        for i in range(Features.shape[0]):\n",
    "            for j in range(Features.shape[1]):\n",
    "                Features[i,j] = 2 * (Features[i,j] - Features[:,j].min()) / (Features[:,j].max() - Features[:,j].min())-1\n",
    "        for i in range(Val_Features.shape[0]):\n",
    "            for j in range(Val_Features.shape[1]):\n",
    "                Val_Features[i,j] = 2 * (Val_Features[i,j] - Val_Features[:,j].min()) / (Val_Features[:,j].max() - Val_Features[:,j].min())-1\n",
    "        scaled_Features = Features\n",
    "        scaled_Val = Val_Features\n",
    "    \n",
    "    if z == 0:\n",
    "        activ_f = \"relu\"\n",
    "    elif z == 1:\n",
    "        activ_f = \"leakyRelu\"\n",
    "    else:\n",
    "        activ_f = \"tanh\"\n",
    "        \n",
    "    train_set = scaled_Features\n",
    "    val_set = scaled_Val\n",
    "    train_label = dummy_y\n",
    "    val_label = dummy_val\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    if z == 1: # first one as regular relu\n",
    "        model.add(Dense(Features.shape[1],input_shape=(Features.shape[1], ), activation=\"relu\", use_bias = True))\n",
    "    else:\n",
    "        model.add(Dense(Features.shape[1],input_shape=(Features.shape[1], ), activation=activ_f, use_bias = True))\n",
    "    model.add(Dropout(a/100))\n",
    "    if z != 1 :\n",
    "        model.add(Dense(c, activation=activ_f))\n",
    "    else:\n",
    "        model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(Dropout(b/100))\n",
    "    if z !=1:\n",
    "        model.add(Dense(d, activation=activ_f))\n",
    "    else:\n",
    "        model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    if h == 0:\n",
    "        optim = \"adam\"\n",
    "    elif h == 1:\n",
    "        optim = \"adadelta\"\n",
    "    elif h == 2:\n",
    "        optim = \"adagrad\"\n",
    "    else:\n",
    "        optim =\"sgd\"\n",
    "    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    model.fit(train_set, train_label, epochs=e, batch_size=f, validation_data = (val_set, val_label))\n",
    "    \n",
    "    score = model.evaluate(val_set, val_label)\n",
    "    score2 = model.evaluate(train_set, train_label)\n",
    "    \n",
    "    score_train = score2[1]\n",
    "    score_test = score[1]\n",
    "    \n",
    "    return score_train, score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def The_Model(Dataset1):\n",
    "    Tree_Model_tuning = pd.DataFrame()\n",
    "    SVM_Model_tuning = pd.DataFrame()\n",
    "    NN_Model_tuning = pd.DataFrame()\n",
    "    \n",
    "    Counter = 0\n",
    "    \n",
    "    for weight in [75, 85, 95, 100]:\n",
    "        \n",
    "        Dataset2, Indicator = prev_season_weight(weight, Dataset1)\n",
    "        \n",
    "        for n in [34,15,5]:\n",
    "            Prev_Games_avg_data = prev_games_avg_data(n, Indicator, Dataset2)\n",
    "            \n",
    "            for m in [34,15,5]:\n",
    "                Prev_Games_avg_data_for_rank, Indicator2 = prev_games_avg_data_for_rank(m, Indicator, Dataset2)\n",
    "                Prev_Games_ranking = prev_games_ranking(Indicator2, Prev_Games_avg_data_for_rank)\n",
    "                \n",
    "                Data = data_merge(Prev_Games_avg_data, Prev_Games_avg_data_for_rank, Prev_Games_ranking, Dataset2).dropna()\n",
    "                \n",
    "                for Modeltype in range(3):#range(3):\n",
    "                    if Modeltype == 0: # Decision Tree\n",
    "                        for g in range(3):\n",
    "                            for k in range(2,15):\n",
    "                                \n",
    "                                score_train, score_test = Tree_Modelling(Data,g,k)\n",
    "                                        \n",
    "                                Tuned = [[Counter, weight, n, m, g, k, score_train, score_test]]\n",
    "                                Tree_Model_tuning = Tree_Model_tuning.append(Tuned)\n",
    "                                print(Counter, weight, n, m, g, k, score_train, score_test)\n",
    "                                                   \n",
    "                                Counter += 1\n",
    "                                    \n",
    "                    elif Modeltype == 1: # SVM\n",
    "                        for g in range(3):\n",
    "                            for k in range(3):\n",
    "                                for p in [3,6,12]:\n",
    "                                    score_train, score_test = SVM_Modelling(Data,g,k,p)\n",
    "                                        \n",
    "                                    Tuned = [[Counter, weight, n, m, g, k, p,score_train, score_test]]\n",
    "                                    SVM_Model_tuning = SVM_Model_tuning.append(Tuned)\n",
    "                                    print(Counter, weight, n, m, g, k, p, score_train, score_test)\n",
    "                                                   \n",
    "                                    Counter += 1\n",
    "                                \n",
    "                    else: # NN\n",
    "                        for a in [20,30,40]:\n",
    "                            for b in [15,25,30]:\n",
    "                                for c in [30,40]:\n",
    "                                    for d in [10,20]:\n",
    "                                        for e in [40,75, 100]:\n",
    "                                            for f in [32,64, 128]:\n",
    "                                                for g in range(3):\n",
    "                                                    for h in range(4):\n",
    "                                                        for z in range(3):\n",
    "                                                            \n",
    "                                                            score_train, score_test = NN_Modelling(Data,a,b,c,d,e,f,g,h,z)\n",
    "                                        \n",
    "                                                            Tuned = [[Counter, weight, n, m, a, b, c, d, e, f, g, h, z, score_train, score_test]]\n",
    "                                                            NN_Model_tuning = NN_Model_tuning.append(Tuned)\n",
    "                                                            print(Counter, weight, n, m, a, b, c, d, e, f, g, h, z, score_train, score_test)\n",
    "                                                   \n",
    "                                                            Counter += 1\n",
    "                                        \n",
    "    Tree_Model_tuning.columns= [\"Counter\",\"prev season weight\",\"prev games incl\",\"prev games for rank\",\"Scaler\",\"Max depth\",\"score_train\",\"score_val\"]\n",
    "    Tree_Model_tuning = Tree_Model_tuning.reset_index(drop=True)\n",
    "    SVM_Model_tuning.columns= [\"Counter\",\"prev season weight\",\"prev games incl\",\"prev games for rank\",\"Scaler\",\"Kernel\",\"Polynomial\",\"score_train\",\"score_val\"]\n",
    "    SVM_Model_tuning = SVM_Model_tuning.reset_index(drop=True)\n",
    "    NN_Model_tuning.columns= [\"Counter\",\"prev season weight\",\"prev games incl\",\"prev games for rank\",\"Dropout rate1\",\"Dropout rate2\",\"Nodes Dense2\",\"Nodes Dense3\",\"Epochs\",\"Batch size\",\"Scaler\",\"Optimizer\",\"Activation Function\",\"score_train\",\"score_val\"]\n",
    "    NN_Model_tuning = NN_Model_tuning.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    return Tree_Model_tuning, SVM_Model_tuning, NN_Model_tuning, Dataset1, Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting the optimal hyperparameters and the dataset with the testset identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree_Model_tuning, SVM_Model_tuning, NN_Model_tuning, Train_Val_Test_dataset, Final_Dataset = The_Model(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Train_Val_Test_dataset.shape, Final_Dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Visualization_Dataset = Final_Dataset[[\"Season start\",\"Gameday\",\"HomeTeam\",\"AwayTeam\",\"Home Goals\",\"Away Goals\",\"Result\",\"Odds H\",\"Odds D\",\"Odds A\"]]\n",
    "\n",
    "# saving data for visualization\n",
    "Visualization_Dataset.to_csv(r'C:\\Users\\### LOCAL PATH ###\\Data_for_visualization.txt', header = True, index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimal hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyper_Parameters_Tree = Tree_Model_tuning.sort_values([\"score_val\",\"score_train\"], ascending = [False,False]).reset_index(drop=True)\n",
    "Hyper_Parameters_Tree.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyper_Parameters_SVM = SVM_Model_tuning.sort_values([\"score_val\",\"score_train\"], ascending = [False,False]).reset_index(drop=True)\n",
    "Hyper_Parameters_SVM.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyper_Parameters_NN = NN_Model_tuning.sort_values([\"score_val\",\"score_train\"], ascending = [False,False]).reset_index(drop=True)\n",
    "Hyper_Parameters_NN.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### saved /extracted the hyperparameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tree\n",
    "Tree_weight = Hyper_Parameters_Tree.loc[0, \"prev season weight\"] \n",
    "Tree_n = Hyper_Parameters_Tree.loc[0, \"prev games incl\"] \n",
    "Tree_m = Hyper_Parameters_Tree.loc[0, \"prev games for rank\"] \n",
    "Tree_Scaler = Hyper_Parameters_Tree.loc[0,\"Scaler\"] \n",
    "Tree_depth = Hyper_Parameters_Tree.loc[0,\"Max depth\"] \n",
    "\n",
    "# SVM\n",
    "SVM_weight = Hyper_Parameters_SVM.loc[0, \"prev season weight\"] \n",
    "SVM_n = Hyper_Parameters_SVM.loc[0, \"prev games incl\"] \n",
    "SVM_m = Hyper_Parameters_SVM.loc[0, \"prev games for rank\"] \n",
    "SVM_Scaler = Hyper_Parameters_SVM.loc[0,\"Scaler\"] \n",
    "SVM_Kernel = Hyper_Parameters_SVM.loc[0,\"Kernel\"] \n",
    "SVM_Polynomial = Hyper_Parameters_SVM.loc[0,\"Polynomial\"] \n",
    "\n",
    "# NN\n",
    "NN_weight = Hyper_Parameters_NN.loc[0, \"prev season weight\"] \n",
    "NN_n = Hyper_Parameters_NN.loc[0, \"prev games incl\"] \n",
    "NN_m = Hyper_Parameters_NN.loc[0, \"prev games for rank\"] \n",
    "NN_Scaler = Hyper_Parameters_NN.loc[0,\"Scaler\"] \n",
    "NN_dropout_rate1 = Hyper_Parameters_NN.loc[0,\"Dropout rate1\"] \n",
    "NN_dropout_rate2 = Hyper_Parameters_NN.loc[0,\"Dropout rate2\"] \n",
    "NN_Nodes_dense2 = Hyper_Parameters_NN.loc[0,\"Nodes Dense2\"] \n",
    "NN_Nodes_dense3 = Hyper_Parameters_NN.loc[0,\"Nodes Dense3\"] \n",
    "NN_Epochs = Hyper_Parameters_NN.loc[0,\"Epochs\"] \n",
    "NN_Batch_size = Hyper_Parameters_NN.loc[0,\"Batch size\"] \n",
    "NN_Optimizer = Hyper_Parameters_NN.loc[0,\"Optimizer\"]\n",
    "NN_Activation_Function = Hyper_Parameters_NN.loc[0,\"Activation Function\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting and scoring Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the best model -> extract hyperparameters, fit the model on the train_val dataset, score the model on the test set\n",
    "\n",
    "# Tree\n",
    "Dataset_step1, Indicator1 = prev_season_weight(Tree_weight, Train_Val_Test_dataset)\n",
    "Prev_Games_avg_data = prev_games_avg_data(Tree_n, Indicator1, Dataset_step1)\n",
    "Prev_Games_avg_data_for_rank, Indicator2 = prev_games_avg_data_for_rank(Tree_m, Indicator1, Dataset_step1)\n",
    "Prev_Games_ranking = prev_games_ranking(Indicator2, Prev_Games_avg_data_for_rank)              \n",
    "Data_for_model_fit = data_merge(Prev_Games_avg_data, Prev_Games_avg_data_for_rank, Prev_Games_ranking, Dataset_step1)\n",
    "\n",
    "\n",
    "#\n",
    "Data_for_model_fit = Data_for_model_fit.sort_values([\"Test set identifier\"], ascending = True)\n",
    "Data_for_model_fit = Data_for_model_fit.reset_index(drop=True)\n",
    "Data_for_model_fit = Data_for_model_fit.drop([\"Test set identifier\"], axis=1).dropna()\n",
    "\n",
    "\n",
    "    \n",
    "Test_set = Data_for_model_fit.iloc[:(round(0.15*Data_for_model_fit.shape[0])),:].reset_index(drop=True)\n",
    "Train_and_Val = Data_for_model_fit.iloc[(round(0.15*Data_for_model_fit.shape[0])):,:].reset_index(drop=True)\n",
    "    \n",
    "Train_and_Val = Train_and_Val.sort_values([\"Val set identifier\"], ascending = True)\n",
    "Train_and_Val = Train_and_Val.reset_index(drop=True)\n",
    "Train_and_Val = Train_and_Val.drop([\"Val set identifier\"], axis=1).dropna()\n",
    "Test_set = Test_set.drop([\"Val set identifier\"], axis=1).dropna()\n",
    "    \n",
    "Val_set = Train_and_Val.iloc[:(round(0.12*Train_and_Val.shape[0])),:].reset_index(drop=True)\n",
    "Train_set = Train_and_Val.iloc[(round(0.12*Train_and_Val.shape[0])):,:].reset_index(drop=True)\n",
    "    \n",
    "# Trainset\n",
    "Tree_train_target = Train_set.loc[:, \"Result\"]\n",
    "Train_Features = Train_set.iloc[:, 7:].values\n",
    "\n",
    "# Valset\n",
    "Tree_val_target = Val_set.loc[:, \"Result\"]\n",
    "Val_Features = Val_set.iloc[:, 7:].values\n",
    "\n",
    "# Testset\n",
    "Tree_test_target = Test_set.loc[:, \"Result\"]\n",
    "Test_Features = Test_set.iloc[:, 7:].values\n",
    "\n",
    "# scaling\n",
    "if Tree_Scaler == 2:\n",
    "    Train_Features = scaleData(Train_Features)\n",
    "    Val_Features = scaleData(Val_Features)\n",
    "    Test_Features = scaleData(Test_Features)\n",
    "elif Tree_Scaler == 1:\n",
    "    Train_Features = scaleData2(Train_Features)\n",
    "    Val_Features = scaleData2(Val_Features)\n",
    "    Test_Features = scaleData2(Test_Features)\n",
    "else:\n",
    "\n",
    "    scaled_Features = Features\n",
    "    scaled_Val = Val_Features\n",
    "\n",
    "Result_Tree = DecisionTreeClassifier(criterion='entropy', max_depth = Tree_depth)  \n",
    "Result_Tree.fit(Train_Features, Tree_train_target)\n",
    "    \n",
    "    \n",
    "Tree_train_score = metrics.accuracy_score(Tree_train_target, Result_Tree.predict(Train_Features))\n",
    "Tree_val_score = metrics.accuracy_score(Tree_val_target, Result_Tree.predict(Val_Features))\n",
    "Tree_test_score = metrics.accuracy_score(Tree_test_target, Result_Tree.predict(Test_Features))\n",
    "\n",
    "Tree_train_prediction = Result_Tree.predict(Train_Features)\n",
    "Tree_val_prediction = Result_Tree.predict(Val_Features)\n",
    "Tree_test_prediction = Result_Tree.predict(Test_Features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting the best model -> extract hyperparameters, fit the model on the train_val dataset, score the model on the test set\n",
    "\n",
    "# SVM \n",
    "Dataset_step1, Indicator1 = prev_season_weight(SVM_weight, Train_Val_Test_dataset)\n",
    "Prev_Games_avg_data = prev_games_avg_data(SVM_n, Indicator1, Dataset_step1)\n",
    "Prev_Games_avg_data_for_rank, Indicator2 = prev_games_avg_data_for_rank(SVM_m, Indicator1, Dataset_step1)\n",
    "Prev_Games_ranking = prev_games_ranking(Indicator2, Prev_Games_avg_data_for_rank)              \n",
    "Data_for_model_fit = data_merge(Prev_Games_avg_data, Prev_Games_avg_data_for_rank, Prev_Games_ranking, Dataset_step1)\n",
    "\n",
    "\n",
    "#\n",
    "Data_for_model_fit = Data_for_model_fit.sort_values([\"Test set identifier\"], ascending = True)\n",
    "Data_for_model_fit = Data_for_model_fit.reset_index(drop=True)\n",
    "Data_for_model_fit = Data_for_model_fit.drop([\"Test set identifier\"], axis=1).dropna()\n",
    "\n",
    "Test_set = Data_for_model_fit.iloc[:(round(0.15*Data_for_model_fit.shape[0])),:].reset_index(drop=True)\n",
    "Train_and_Val = Data_for_model_fit.iloc[(round(0.15*Data_for_model_fit.shape[0])):,:].reset_index(drop=True)\n",
    "\n",
    "Train_and_Val = Train_and_Val.sort_values([\"Val set identifier\"], ascending = True)\n",
    "Train_and_Val = Train_and_Val.reset_index(drop=True)\n",
    "Train_and_Val = Train_and_Val.drop([\"Val set identifier\"], axis=1).dropna()\n",
    "Test_set = Test_set.drop([\"Val set identifier\"], axis=1).dropna()\n",
    "    \n",
    "Val_set = Train_and_Val.iloc[:(round(0.12*Train_and_Val.shape[0])),:].reset_index(drop=True)\n",
    "Train_set = Train_and_Val.iloc[(round(0.12*Train_and_Val.shape[0])):,:].reset_index(drop=True)\n",
    "    \n",
    "# Trainset\n",
    "SVM_train_target = Train_set.loc[:, \"Result\"]\n",
    "Train_Features = Train_set.iloc[:, 7:]\n",
    "Train_Features = Train_Features.values\n",
    "\n",
    "# Valset\n",
    "SVM_val_target = Val_set.loc[:, \"Result\"]\n",
    "Val_Features = Val_set.iloc[:, 7:].values\n",
    "    \n",
    "# Testset\n",
    "SVM_test_target = Test_set.loc[:, \"Result\"]\n",
    "Test_Features = Test_set.iloc[:, 7:]\n",
    "Test_Features = Test_Features.values\n",
    "\n",
    "# scaling\n",
    "if SVM_Scaler == 2:\n",
    "    Train_Features = scaleData(Train_Features)\n",
    "    Val_Features = scaleData(Val_Features)\n",
    "    Test_Features = scaleData(Test_Features)\n",
    "elif SVM_Scaler == 1:\n",
    "    Train_Features = scaleData2(Train_Features)\n",
    "    Val_Features = scaleData2(Val_Features)\n",
    "    Test_Features = scaleData2(Test_Features)\n",
    "else:\n",
    "    for i in range(Train_Features.shape[0]):\n",
    "        for j in range(Train_Features.shape[1]):\n",
    "            Train_Features[i,j] = 2 * (Train_Features[i,j] - Train_Features[:,j].min()) / (Train_Features[:,j].max() - Train_Features[:,j].min())-1\n",
    "    for i in range(Val_Features.shape[0]):\n",
    "        for j in range(Val_Features.shape[1]):\n",
    "            Val_Features[i,j] = 2 * (Val_Features[i,j] - Val_Features[:,j].min()) / (Val_Features[:,j].max() - Val_Features[:,j].min())-1\n",
    "    for i in range(Test_Features.shape[0]):\n",
    "        for j in range(Test_Features.shape[1]):\n",
    "            Test_Features[i,j] = 2 * (Test_Features[i,j] - Test_Features[:,j].min()) / (Test_Features[:,j].max() - Test_Features[:,j].min())-1\n",
    "    \n",
    "\n",
    "if SVM_Kernel == 0:\n",
    "    SVM_clf = svm.SVC(kernel='linear',gamma='auto')\n",
    "elif SVM_Kernel == 1:\n",
    "    SVM_clf= svm.SVC(kernel ='poly', degree= SVM_Polynomial, gamma='auto')\n",
    "else:\n",
    "    SVM_clf = svm.SVC(kernel='rbf', gamma = 'auto')\n",
    "    \n",
    "    \n",
    "SVM_clf.fit(Train_Features, SVM_train_target)\n",
    "    \n",
    "SVM_train_score = metrics.accuracy_score(SVM_train_target, SVM_clf.predict(Train_Features))\n",
    "SVM_val_score = metrics.accuracy_score(SVM_val_target, SVM_clf.predict(Val_Features))\n",
    "SVM_test_score = metrics.accuracy_score(SVM_test_target, SVM_clf.predict(Test_Features))\n",
    "        \n",
    "SVM_train_prediction = SVM_clf.predict(Train_Features)\n",
    "SVM_val_prediction = SVM_clf.predict(Val_Features)\n",
    "SVM_test_prediction = SVM_clf.predict(Test_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the best model -> extract hyperparameters, fit the model on the train_val dataset, score the model on the test set\n",
    "\n",
    "# NN\n",
    "Dataset_step1, Indicator1 = prev_season_weight(NN_weight, Train_Val_Test_dataset)\n",
    "Prev_Games_avg_data = prev_games_avg_data(NN_n, Indicator1, Dataset_step1)\n",
    "Prev_Games_avg_data_for_rank, Indicator2 = prev_games_avg_data_for_rank(NN_m, Indicator1, Dataset_step1)\n",
    "Prev_Games_ranking = prev_games_ranking(Indicator2, Prev_Games_avg_data_for_rank)              \n",
    "Data_for_model_fit = data_merge(Prev_Games_avg_data, Prev_Games_avg_data_for_rank, Prev_Games_ranking, Dataset_step1)\n",
    "\n",
    "\n",
    "#\n",
    "Data_for_model_fit = Data_for_model_fit.sort_values([\"Test set identifier\"], ascending = True)\n",
    "Data_for_model_fit = Data_for_model_fit.reset_index(drop=True)\n",
    "Data_for_model_fit = Data_for_model_fit.drop([\"Test set identifier\"], axis=1).dropna()\n",
    "\n",
    "Test_set = Data_for_model_fit.iloc[:(round(0.15*Data_for_model_fit.shape[0])),:].reset_index(drop=True)\n",
    "Train_and_Val = Data_for_model_fit.iloc[(round(0.15*Data_for_model_fit.shape[0])):,:].reset_index(drop=True)\n",
    "    \n",
    "Train_and_Val = Train_and_Val.sort_values([\"Val set identifier\"], ascending = True)\n",
    "Train_and_Val = Train_and_Val.reset_index(drop=True)\n",
    "Train_and_Val = Train_and_Val.drop([\"Val set identifier\"], axis=1).dropna()\n",
    "Test_set = Test_set.drop([\"Val set identifier\"], axis=1).dropna()\n",
    "    \n",
    "Val_set = Train_and_Val.iloc[:(round(0.12*Train_and_Val.shape[0])),:].reset_index(drop=True)\n",
    "Train_set = Train_and_Val.iloc[(round(0.12*Train_and_Val.shape[0])):,:].reset_index(drop=True)\n",
    "\n",
    "# Trainset\n",
    "Train_Target = Train_set.loc[:, \"Result\"]\n",
    "Train_Features = Train_set.iloc[:, 7:]\n",
    "Train_Features = Train_Features.values\n",
    "    \n",
    "# Valset\n",
    "Val_Target = Val_set.loc[:, \"Result\"]\n",
    "Val_Features = Val_set.iloc[:, 7:].values\n",
    "\n",
    "# Testset\n",
    "Test_Target = Test_set.loc[:, \"Result\"]\n",
    "Test_Features = Test_set.iloc[:, 7:]\n",
    "Test_Features = Test_Features.values\n",
    "    \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Train_Target)\n",
    "encoded_Y = encoder.transform(Train_Target)\n",
    "NN_train_target = np_utils.to_categorical(encoded_Y)\n",
    "    \n",
    "encoded_Val = encoder.transform(Val_Target)\n",
    "NN_val_target = np_utils.to_categorical(encoded_Val)\n",
    "    \n",
    "encoded_Y2 = encoder.transform(Test_Target)\n",
    "NN_test_target = np_utils.to_categorical(encoded_Y2)\n",
    "    \n",
    "# scaling\n",
    "\n",
    "if NN_Scaler == 2:\n",
    "    Train_Features = scaleData(Train_Features)\n",
    "    Val_Features = scaleData(Val_Features)\n",
    "    Test_Features = scaleData(Test_Features)\n",
    "elif NN_Scaler == 1:\n",
    "    Train_Features = scaleData2(Train_Features)\n",
    "    Val_Features = scaleData2(Val_Features)\n",
    "    Test_Features = scaleData2(Test_Features)\n",
    "else:\n",
    "    for i in range(Train_Features.shape[0]):\n",
    "        for j in range(Train_Features.shape[1]):\n",
    "            Train_Features[i,j] = 2 * (Train_Features[i,j] - Train_Features[:,j].min()) / (Train_Features[:,j].max() - Train_Features[:,j].min())-1\n",
    "    for i in range(Val_Features.shape[0]):\n",
    "        for j in range(Val_Features.shape[1]):\n",
    "            Val_Features[i,j] = 2 * (Val_Features[i,j] - Val_Features[:,j].min()) / (Val_Features[:,j].max() - Val_Features[:,j].min())-1\n",
    "    for i in range(Test_Features.shape[0]):\n",
    "        for j in range(Test_Features.shape[1]):\n",
    "            Test_Features[i,j] = 2 * (Test_Features[i,j] - Test_Features[:,j].min()) / (Test_Features[:,j].max() - Test_Features[:,j].min())-1\n",
    "    \n",
    "        \n",
    "if NN_Activation_Function == 0:\n",
    "    activ_f = \"relu\"\n",
    "elif NN_Activation_Function == 1:\n",
    "    activ_f = \"leakyRelu\"\n",
    "else:\n",
    "    activ_f = \"tanh\"\n",
    "        \n",
    "NN_model = Sequential()\n",
    "if NN_Activation_Function == 1:\n",
    "    NN_model.add(Dense(Train_Features.shape[1],input_shape=(Train_Features.shape[1], ), activation=\"relu\", \n",
    "                    use_bias = True))\n",
    "else:\n",
    "    NN_model.add(Dense(Train_Features.shape[1],input_shape=(Train_Features.shape[1], ), activation=activ_f, \n",
    "                    use_bias = True))\n",
    "NN_model.add(Dropout(NN_dropout_rate1/100))\n",
    "if NN_Activation_Function != 1 :\n",
    "    NN_model.add(Dense(NN_Nodes_dense2, activation=activ_f))\n",
    "else:\n",
    "    NN_model.add(LeakyReLU(alpha = 0.1))\n",
    "NN_model.add(Dropout(NN_dropout_rate2/100))\n",
    "if NN_Activation_Function !=1:\n",
    "    NN_model.add(Dense(NN_Nodes_dense3, activation=activ_f))\n",
    "else:\n",
    "    NN_model.add(LeakyReLU(alpha = 0.1))\n",
    "NN_model.add(Dense(3, activation='softmax'))\n",
    "   \n",
    "if NN_Optimizer == 0:\n",
    "    optim = \"adam\"\n",
    "elif NN_Optimizer == 1:\n",
    "    optim = \"adadelta\"\n",
    "elif NN_Optimizer == 2:\n",
    "    optim = \"adagrad\"\n",
    "else:\n",
    "    optim =\"sgd\"\n",
    "\n",
    "    \n",
    "NN_model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "NN_model.fit(Train_Features, NN_train_target, epochs=NN_Epochs, batch_size=NN_Batch_size)\n",
    "    \n",
    "score = NN_model.evaluate(Test_Features, NN_test_target)\n",
    "score_val = NN_model.evaluate(Val_Features, NN_val_target)\n",
    "score2 = NN_model.evaluate(Train_Features, NN_train_target)\n",
    "    \n",
    "NN_train_score = score2[1]\n",
    "NN_val_score = score_val[1]\n",
    "NN_test_score = score[1]\n",
    "    \n",
    "NN_train_prediction = NN_model.predict(Train_Features)\n",
    "NN_val_prediction = NN_model.predict(Val_Features)\n",
    "NN_test_prediction = NN_model.predict(Test_Features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores\n",
    "print(Tree_test_score, SVM_test_score, NN_test_score)\n",
    "print(Tree_val_score,SVM_val_score, NN_val_score)\n",
    "print(Tree_train_score, SVM_train_score, NN_train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Strategy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The models 'like' and 'dislike' certain Teams, which materializes in differences of successful predictions. Using the validation set, here, the optimal number of Teams to pick from what the individual model does will be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since the NN outputs \"3 columns with probabilities\" \n",
    "\n",
    "NN_test_Result_prediction = pd.DataFrame(columns=[\"NN Prediction\"])\n",
    "\n",
    "for i in range(Tree_test_target.shape[0]):\n",
    "    if (NN_test_prediction[i,0] > NN_test_prediction[i,1]) & (NN_test_prediction[i,0] >NN_test_prediction[i,2]):\n",
    "        NN_test_Result_prediction.loc[i,\"NN Prediction\"] = \"A\"\n",
    "        \n",
    "    elif (NN_test_prediction[i,1] > NN_test_prediction[i,0]) & (NN_test_prediction[i,1] >NN_test_prediction[i,2]):\n",
    "        NN_test_Result_prediction.loc[i,\"NN Prediction\"] = \"D\"    \n",
    "    \n",
    "    elif (NN_test_prediction[i,2] > NN_test_prediction[i,0]) & (NN_test_prediction[i,2] >NN_test_prediction[i,1]):\n",
    "        NN_test_Result_prediction.loc[i,\"NN Prediction\"] = \"H\"\n",
    "        \n",
    "    else:\n",
    "        NN_test_Result_prediction.loc[i,\"NN Prediction\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_val_Result_prediction = pd.DataFrame(columns=[\"NN Prediction\"])\n",
    "\n",
    "for i in range(NN_val_target.shape[0]):\n",
    "    if (NN_val_prediction[i,0] > NN_val_prediction[i,1]) & (NN_val_prediction[i,0] >NN_val_prediction[i,2]):\n",
    "        NN_val_Result_prediction.loc[i,\"NN Prediction\"] = \"A\"\n",
    "        \n",
    "    elif (NN_val_prediction[i,1] > NN_val_prediction[i,0]) & (NN_val_prediction[i,1] >NN_val_prediction[i,2]):\n",
    "        NN_val_Result_prediction.loc[i,\"NN Prediction\"] = \"D\"    \n",
    "    \n",
    "    elif (NN_val_prediction[i,2] > NN_val_prediction[i,0]) & (NN_val_prediction[i,2] >NN_val_prediction[i,1]):\n",
    "        NN_val_Result_prediction.loc[i,\"NN Prediction\"] = \"H\"\n",
    "        \n",
    "    else:\n",
    "        NN_val_Result_prediction.loc[i,\"NN Prediction\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_train_Result_prediction = pd.DataFrame(columns=[\"NN Prediction\"])\n",
    "\n",
    "for i in range(NN_train_target.shape[0]):\n",
    "    if (NN_train_prediction[i,0] > NN_train_prediction[i,1]) & (NN_train_prediction[i,0] >NN_train_prediction[i,2]):\n",
    "        NN_train_Result_prediction.loc[i,\"NN Prediction\"] = \"A\"\n",
    "        \n",
    "    elif (NN_train_prediction[i,1] > NN_train_prediction[i,0]) & (NN_train_prediction[i,1] >NN_train_prediction[i,2]):\n",
    "        NN_train_Result_prediction.loc[i,\"NN Prediction\"] = \"D\"    \n",
    "    \n",
    "    elif (NN_train_prediction[i,2] > NN_train_prediction[i,0]) & (NN_train_prediction[i,2] >NN_train_prediction[i,1]):\n",
    "        NN_train_Result_prediction.loc[i,\"NN Prediction\"] = \"H\"\n",
    "        \n",
    "    else:\n",
    "        NN_train_Result_prediction.loc[i,\"NN Prediction\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Cells below allow to calculate the accuracy of a strategy that, for a given model and predicted winners /losers, only picks those which involve picking a team / picking against a team, that worked well in the training set. The number of teams is determined based on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 1\n",
    "Validation_evaluation = pd.DataFrame(columns=[\"Actual\",\"Tree Prediction\",\"SVM Prediction\",\"NN Prediction\"])\n",
    "for i in range(Tree_val_target.shape[0]):\n",
    "    Validation_evaluation.loc[i,\"Actual\"] = Tree_val_target[i]\n",
    "    Validation_evaluation.loc[i,\"Tree Prediction\"] = Tree_val_prediction[i]\n",
    "    Validation_evaluation.loc[i,\"SVM Prediction\"] = SVM_val_prediction[i]\n",
    "    Validation_evaluation.loc[i,\"NN Prediction\"] = NN_val_Result_prediction.loc[i,\"NN Prediction\"]\n",
    "    \n",
    "Validation_evaluation[\"Home Team\"] = Val_set[\"HomeTeam\"]\n",
    "Validation_evaluation[\"Away Team\"] = Val_set[\"AwayTeam\"]\n",
    "Validation_evaluation[\"Democracy\"] = np.nan\n",
    "Validation_evaluation[\"Unanimous\"] = np.nan\n",
    "Validation_evaluation[\"Fav Strategy\"] = np.nan\n",
    "\n",
    "for i in range(Validation_evaluation.shape[0]):\n",
    "    \n",
    "    if (Validation_evaluation.loc[i,\"Tree Prediction\"] == Validation_evaluation.loc[i,\"SVM Prediction\"])&(Validation_evaluation.loc[i,\"Tree Prediction\"] == Validation_evaluation.loc[i,\"NN Prediction\"]):\n",
    "        Validation_evaluation.loc[i,\"Unanimous\"] = Validation_evaluation.loc[i,\"Tree Prediction\"]\n",
    "    \n",
    "    #\n",
    "    if (Validation_evaluation.loc[i,\"Tree Prediction\"] == Validation_evaluation.loc[i,\"SVM Prediction\"])&(Validation_evaluation.loc[i,\"Tree Prediction\"] != Validation_evaluation.loc[i,\"NN Prediction\"]):\n",
    "        Validation_evaluation.loc[i,\"Democracy\"] = Validation_evaluation.loc[i,\"Tree Prediction\"]\n",
    "    elif (Validation_evaluation.loc[i,\"Tree Prediction\"] != Validation_evaluation.loc[i,\"SVM Prediction\"])&(Validation_evaluation.loc[i,\"Tree Prediction\"] == Validation_evaluation.loc[i,\"NN Prediction\"]):\n",
    "        Validation_evaluation.loc[i,\"Democracy\"] = Validation_evaluation.loc[i,\"Tree Prediction\"]\n",
    "    elif (Validation_evaluation.loc[i,\"Tree Prediction\"] != Validation_evaluation.loc[i,\"SVM Prediction\"])&(Validation_evaluation.loc[i,\"SVM Prediction\"] == Validation_evaluation.loc[i,\"NN Prediction\"]):\n",
    "        Validation_evaluation.loc[i,\"Democracy\"] = Validation_evaluation.loc[i,\"SVM Prediction\"]\n",
    "    elif (Validation_evaluation.loc[i,\"Tree Prediction\"] == Validation_evaluation.loc[i,\"SVM Prediction\"])&(Validation_evaluation.loc[i,\"Tree Prediction\"] == Validation_evaluation.loc[i,\"NN Prediction\"]):\n",
    "        Validation_evaluation.loc[i,\"Democracy\"] = Validation_evaluation.loc[i,\"Tree Prediction\"]\n",
    "    #\n",
    "    Validation_evaluation.loc[i,\"Fav Strategy\"] = np.where((Val_set.loc[i,\"Odds H\"] < Val_set.loc[i,\"Odds A\"]) & (Val_set.loc[i,\"Odds H\"] < Val_set.loc[i,\"Odds D\"]), \"H\", Validation_evaluation.loc[i,\"Fav Strategy\"])\n",
    "    Validation_evaluation.loc[i,\"Fav Strategy\"] = np.where((Val_set.loc[i,\"Odds A\"] < Val_set.loc[i,\"Odds D\"]) & (Val_set.loc[i,\"Odds A\"] < Val_set.loc[i,\"Odds D\"]), \"A\", Validation_evaluation.loc[i,\"Fav Strategy\"])\n",
    "    Validation_evaluation.loc[i,\"Fav Strategy\"] = np.where((Val_set.loc[i,\"Odds D\"] < Val_set.loc[i,\"Odds H\"]) & (Val_set.loc[i,\"Odds D\"] < Val_set.loc[i,\"Odds A\"]), \"D\", Validation_evaluation.loc[i,\"Fav Strategy\"])\n",
    "    \n",
    "    Validation_evaluation.loc[i,\"Odds H\"] = Val_set.loc[i,\"Odds H\"]\n",
    "    Validation_evaluation.loc[i,\"Odds D\"] = Val_set.loc[i,\"Odds D\"]\n",
    "    Validation_evaluation.loc[i,\"Odds A\"] = Val_set.loc[i,\"Odds A\"]\n",
    "    \n",
    "    \n",
    "Validation_evaluation = Validation_evaluation[[\"Home Team\",\"Away Team\", \"Actual\", \"Tree Prediction\", \"SVM Prediction\",\"NN Prediction\",\"Democracy\", \"Unanimous\",\"Fav Strategy\",\"Odds H\",\"Odds D\",\"Odds A\"]]\n",
    "Validation_evaluation[\"Pick Home\"] = \"H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 2\n",
    "Accuracy_Ranker_by_Team_prep = pd.DataFrame(columns=[\"Actual\",\"Tree Prediction\",\"SVM Prediction\",\"NN Prediction\"])\n",
    "for i in range(Tree_train_target.shape[0]):\n",
    "    Accuracy_Ranker_by_Team_prep.loc[i,\"Actual\"] = Tree_train_target[i]\n",
    "    Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"] = Tree_train_prediction[i]\n",
    "    Accuracy_Ranker_by_Team_prep.loc[i,\"SVM Prediction\"] = SVM_train_prediction[i]\n",
    "    Accuracy_Ranker_by_Team_prep.loc[i,\"NN Prediction\"] = NN_train_Result_prediction.loc[i,\"NN Prediction\"]\n",
    "    \n",
    "Accuracy_Ranker_by_Team_prep[\"Home Team\"] = Train_set[\"HomeTeam\"]\n",
    "Accuracy_Ranker_by_Team_prep[\"Away Team\"] = Train_set[\"AwayTeam\"]\n",
    "Accuracy_Ranker_by_Team_prep[\"Democracy\"] = np.nan\n",
    "Accuracy_Ranker_by_Team_prep[\"Unanimous\"] = np.nan\n",
    "Accuracy_Ranker_by_Team_prep[\"Fav Strategy\"] = np.nan\n",
    "\n",
    "for i in range(Accuracy_Ranker_by_Team_prep.shape[0]):\n",
    "    \n",
    "    if (Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"] == Accuracy_Ranker_by_Team_prep.loc[i,\"SVM Prediction\"])&(Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"] == Accuracy_Ranker_by_Team_prep.loc[i,\"NN Prediction\"]):\n",
    "        Accuracy_Ranker_by_Team_prep.loc[i,\"Unanimous\"] = Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"]\n",
    "    \n",
    "    #\n",
    "    if (Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"] == Accuracy_Ranker_by_Team_prep.loc[i,\"SVM Prediction\"])&(Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"] != Accuracy_Ranker_by_Team_prep.loc[i,\"NN Prediction\"]):\n",
    "        Accuracy_Ranker_by_Team_prep.loc[i,\"Democracy\"] = Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"]\n",
    "    elif (Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"] != Accuracy_Ranker_by_Team_prep.loc[i,\"SVM Prediction\"])&(Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"] == Accuracy_Ranker_by_Team_prep.loc[i,\"NN Prediction\"]):\n",
    "        Accuracy_Ranker_by_Team_prep.loc[i,\"Democracy\"] = Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"]\n",
    "    elif (Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"] != Accuracy_Ranker_by_Team_prep.loc[i,\"SVM Prediction\"])&(Accuracy_Ranker_by_Team_prep.loc[i,\"SVM Prediction\"] == Accuracy_Ranker_by_Team_prep.loc[i,\"NN Prediction\"]):\n",
    "        Accuracy_Ranker_by_Team_prep.loc[i,\"Democracy\"] = Accuracy_Ranker_by_Team_prep.loc[i,\"SVM Prediction\"]\n",
    "    elif (Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"] == Accuracy_Ranker_by_Team_prep.loc[i,\"SVM Prediction\"])&(Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"] == Accuracy_Ranker_by_Team_prep.loc[i,\"NN Prediction\"]):\n",
    "        Accuracy_Ranker_by_Team_prep.loc[i,\"Democracy\"] = Accuracy_Ranker_by_Team_prep.loc[i,\"Tree Prediction\"]\n",
    "    #\n",
    "    Accuracy_Ranker_by_Team_prep.loc[i,\"Fav Strategy\"] = np.where((Train_set.loc[i,\"Odds H\"] < Train_set.loc[i,\"Odds A\"]) & (Train_set.loc[i,\"Odds H\"] < Train_set.loc[i,\"Odds D\"]), \"H\", Accuracy_Ranker_by_Team_prep.loc[i,\"Fav Strategy\"])\n",
    "    Accuracy_Ranker_by_Team_prep.loc[i,\"Fav Strategy\"] = np.where((Train_set.loc[i,\"Odds A\"] < Train_set.loc[i,\"Odds D\"]) & (Train_set.loc[i,\"Odds A\"] < Train_set.loc[i,\"Odds D\"]), \"A\", Accuracy_Ranker_by_Team_prep.loc[i,\"Fav Strategy\"])\n",
    "    Accuracy_Ranker_by_Team_prep.loc[i,\"Fav Strategy\"] = np.where((Train_set.loc[i,\"Odds D\"] < Train_set.loc[i,\"Odds H\"]) & (Train_set.loc[i,\"Odds D\"] < Train_set.loc[i,\"Odds A\"]), \"D\", Accuracy_Ranker_by_Team_prep.loc[i,\"Fav Strategy\"])\n",
    "    \n",
    "Accuracy_Ranker_by_Team_prep = Accuracy_Ranker_by_Team_prep[[\"Home Team\",\"Away Team\", \"Actual\", \"Tree Prediction\", \"SVM Prediction\",\"NN Prediction\",\"Democracy\", \"Unanimous\",\"Fav Strategy\"]]\n",
    "Accuracy_Ranker_by_Team_prep[\"Pick Home\"] = \"H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accuracy = pd.DataFrame(columns=[\"Tree Prediction\",\"SVM Prediction\",\"NN Prediction\",\"Democracy\",\"Unanimous\",\"Fav Strategy\",\"Pick Home\"])\n",
    "\n",
    "for i in range(Accuracy_Ranker_by_Team_prep.shape[0]):\n",
    "    for column in Accuracy.columns:\n",
    "        if str(Accuracy_Ranker_by_Team_prep.loc[i,column]) == \"nan\":\n",
    "            Accuracy.loc[i,column] = np.nan\n",
    "        \n",
    "        else:\n",
    "            Accuracy.loc[i,column] = np.where(Accuracy_Ranker_by_Team_prep.loc[i,\"Actual\"] == Accuracy_Ranker_by_Team_prep.loc[i,column],1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = Accuracy.rename(columns={\"Tree Prediction\":\"Tree Acc\",\"SVM Prediction\":\"SVM Acc\",\"NN Prediction\":\"NN Acc\",\"Democracy\":\"Democracy Acc\",\"Unanimous\":\"Unanimous Acc\",\"Fav Strategy\":\"Fav Acc\",\"Pick Home\":\"HP Acc\"})\n",
    "Data = pd.concat([Accuracy_Ranker_by_Team_prep[[\"Home Team\",\"Away Team\",\"Tree Prediction\",\"SVM Prediction\",\"NN Prediction\",\"Democracy\",\"Unanimous\",\"Fav Strategy\",\"Pick Home\"]], Accuracy[[\"Tree Acc\",\"SVM Acc\",\"NN Acc\", \"Democracy Acc\",\"Unanimous Acc\",\"Fav Acc\",\"HP Acc\"]]], axis=1)\n",
    "Teams_list = list(set(list(Data[\"Home Team\"])+(list(Data[\"Away Team\"]))))\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which teams were accurately predicted to win / not to win:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data_Team_Bias = pd.DataFrame(columns=[\"Team\",\"Tree for\",\"Tree for correct\",\"Tree against\",\"Tree against correct\",\n",
    "                                      \"SVM for\",\"SVM for correct\",\"SVM against\",\"SVM against correct\",\n",
    "                                      \"NN for\",\"NN for correct\",\"NN against\",\"NN against correct\",\n",
    "                                       \"Democracy for\",\"Democracy for correct\",\"Democracy against\",\"Democracy against correct\",\n",
    "                        \"Unanimous for\", \"Unanimous for correct\", \"Unanimous against\", \"Unanimous against correct\",\n",
    "                                   \"Favorite for\", \"Favorite for correct\", \"Favorite against\", \"Favorite against correct\",\n",
    "                                       \"HP for\", \"HP for correct\", \"HP against\", \"HP against correct\"])\n",
    "\n",
    "i = 0\n",
    "for Team in Teams_list:\n",
    "    df1 = Data.loc[Data[\"Home Team\"] == Team]\n",
    "    df2 = Data.loc[Data[\"Away Team\"] == Team]\n",
    "    \n",
    "    for model in [\"Tree\",\"SVM\",\"NN\",\"Democracy\",\"Unanimous\",\"Favorite\",\"HP\"]:\n",
    "        if model == \"Tree\": \n",
    "            T_H_for = df1.loc[df1[\"Tree Prediction\"]==\"H\"].shape[0]\n",
    "            T_H_against = df1.loc[df1[\"Tree Prediction\"]!=\"H\"].shape[0]\n",
    "            \n",
    "            T_H_for_correct = df1.loc[df1[\"Tree Prediction\"]==\"H\"][\"Tree Acc\"].sum()\n",
    "            T_H_against_correct = df1.loc[df1[\"Tree Prediction\"]!=\"H\"][\"Tree Acc\"].sum()\n",
    "            \n",
    "            ###\n",
    "            T_A_for = df2.loc[df2[\"Tree Prediction\"]==\"A\"].shape[0]\n",
    "            T_A_against = df2.loc[df2[\"Tree Prediction\"]!=\"A\"].shape[0]\n",
    "            \n",
    "            T_A_for_correct = df2.loc[df2[\"Tree Prediction\"]==\"A\"][\"Tree Acc\"].sum()\n",
    "            T_A_against_correct = df2.loc[df2[\"Tree Prediction\"]!=\"A\"][\"Tree Acc\"].sum()\n",
    "            \n",
    "            T_for = T_H_for + T_A_for\n",
    "            T_for_corr = T_H_for_correct + T_A_for_correct\n",
    "            T_against = T_H_against + T_A_against\n",
    "            T_against_corr = T_H_against_correct + T_A_against_correct\n",
    "            \n",
    "        elif model == \"SVM\": \n",
    "            S_H_for = df1.loc[df1[\"SVM Prediction\"]==\"H\"].shape[0]\n",
    "            S_H_against = df1.loc[df1[\"SVM Prediction\"]!=\"H\"].shape[0]\n",
    "            \n",
    "            S_H_for_correct = df1.loc[df1[\"SVM Prediction\"]==\"H\"][\"Tree Acc\"].sum()\n",
    "            S_H_against_correct = df1.loc[df1[\"SVM Prediction\"]!=\"H\"][\"Tree Acc\"].sum()\n",
    "            \n",
    "            ###\n",
    "            S_A_for = df2.loc[df2[\"SVM Prediction\"]==\"A\"].shape[0]\n",
    "            S_A_against = df2.loc[df2[\"SVM Prediction\"]!=\"A\"].shape[0]\n",
    "            \n",
    "            S_A_for_correct = df2.loc[df2[\"SVM Prediction\"]==\"A\"][\"Tree Acc\"].sum()\n",
    "            S_A_against_correct = df2.loc[df2[\"SVM Prediction\"]!=\"A\"][\"Tree Acc\"].sum()\n",
    "            \n",
    "            S_for = S_H_for + S_A_for\n",
    "            S_for_corr = S_H_for_correct + S_A_for_correct\n",
    "            S_against = S_H_against + S_A_against\n",
    "            S_against_corr = S_H_against_correct + S_A_against_correct\n",
    "            \n",
    "        elif model == \"NN\": \n",
    "            N_H_for = df1.loc[df1[\"NN Prediction\"]==\"H\"].shape[0]\n",
    "            N_H_against = df1.loc[df1[\"NN Prediction\"]!=\"H\"].shape[0]\n",
    "            \n",
    "            N_H_for_correct = df1.loc[df1[\"NN Prediction\"]==\"H\"][\"Tree Acc\"].sum()\n",
    "            N_H_against_correct = df1.loc[df1[\"NN Prediction\"]!=\"H\"][\"Tree Acc\"].sum()\n",
    "            \n",
    "            ###\n",
    "            N_A_for = df2.loc[df2[\"NN Prediction\"]==\"A\"].shape[0]\n",
    "            N_A_against = df2.loc[df2[\"NN Prediction\"]!=\"A\"].shape[0]\n",
    "            \n",
    "            N_A_for_correct = df2.loc[df2[\"NN Prediction\"]==\"A\"][\"Tree Acc\"].sum()\n",
    "            N_A_against_correct = df2.loc[df2[\"NN Prediction\"]!=\"A\"][\"Tree Acc\"].sum()\n",
    "            \n",
    "            N_for = N_H_for + N_A_for\n",
    "            N_for_corr = N_H_for_correct + N_A_for_correct\n",
    "            N_against = N_H_against + N_A_against\n",
    "            N_against_corr = N_H_against_correct + N_A_against_correct  \n",
    "        \n",
    "        elif model == \"Democracy\": \n",
    "            Democracy_H_for = df1.loc[df1[\"Democracy\"]==\"H\"].shape[0]\n",
    "            Democracy_H_against = df1.loc[df1[\"Democracy\"]!=\"H\"].shape[0]\n",
    "            \n",
    "            Democracy_H_for_correct = df1.loc[df1[\"Democracy\"]==\"H\"][\"Democracy Acc\"].sum()\n",
    "            Democracy_H_against_correct = df1.loc[df1[\"Democracy\"]!=\"H\"][\"Democracy Acc\"].sum()\n",
    "            \n",
    "            ###\n",
    "            Democracy_A_for = df2.loc[df2[\"Democracy\"]==\"A\"].shape[0]\n",
    "            Democracy_A_against = df2.loc[df2[\"Democracy\"]!=\"A\"].shape[0]\n",
    "            \n",
    "            Democracy_A_for_correct = df2.loc[df2[\"Democracy\"]==\"A\"][\"Democracy Acc\"].sum()\n",
    "            Democracy_A_against_correct = df2.loc[df2[\"Democracy\"]!=\"A\"][\"Democracy Acc\"].sum()\n",
    "            \n",
    "            Democracy_for = Democracy_H_for + Democracy_A_for\n",
    "            Democracy_for_corr = Democracy_H_for_correct + Democracy_A_for_correct\n",
    "            Democracy_against = Democracy_H_against + Democracy_A_against\n",
    "            Democracy_against_corr = Democracy_H_against_correct + Democracy_A_against_correct\n",
    "        \n",
    "        elif model == \"Unanimous\": \n",
    "            Unanimous_H_for = df1.loc[df1[\"Unanimous\"]==\"H\"].shape[0]\n",
    "            Unanimous_H_against = df1.loc[df1[\"Unanimous\"]!=\"H\"].shape[0]\n",
    "            \n",
    "            Unanimous_H_for_correct = df1.loc[df1[\"Unanimous\"]==\"H\"][\"Unanimous Acc\"].sum()\n",
    "            Unanimous_H_against_correct = df1.loc[df1[\"Unanimous\"]!=\"H\"][\"Unanimous Acc\"].sum()\n",
    "            \n",
    "            ###\n",
    "            Unanimous_A_for = df2.loc[df2[\"Unanimous\"]==\"A\"].shape[0]\n",
    "            Unanimous_A_against = df2.loc[df2[\"Unanimous\"]!=\"A\"].shape[0]\n",
    "            \n",
    "            Unanimous_A_for_correct = df2.loc[df2[\"Unanimous\"]==\"A\"][\"Unanimous Acc\"].sum()\n",
    "            Unanimous_A_against_correct = df2.loc[df2[\"Unanimous\"]!=\"A\"][\"Unanimous Acc\"].sum()\n",
    "            \n",
    "            Unanimous_for = Unanimous_H_for + Unanimous_A_for\n",
    "            Unanimous_for_corr = Unanimous_H_for_correct + Unanimous_A_for_correct\n",
    "            Unanimous_against = Unanimous_H_against + Unanimous_A_against\n",
    "            Unanimous_against_corr = Unanimous_H_against_correct + Unanimous_A_against_correct\n",
    "        \n",
    "        elif model == \"Favorite\": \n",
    "            Favorite_H_for = df1.loc[df1[\"Fav Strategy\"]==\"H\"].shape[0]\n",
    "            Favorite_H_against = df1.loc[df1[\"Fav Strategy\"]!=\"H\"].shape[0]\n",
    "            \n",
    "            Favorite_H_for_correct = df1.loc[df1[\"Fav Strategy\"]==\"H\"][\"Fav Acc\"].sum()\n",
    "            Favorite_H_against_correct = df1.loc[df1[\"Fav Strategy\"]!=\"H\"][\"Fav Acc\"].sum()\n",
    "            \n",
    "            ###\n",
    "            Favorite_A_for = df2.loc[df2[\"Fav Strategy\"]==\"A\"].shape[0]\n",
    "            Favorite_A_against = df2.loc[df2[\"Fav Strategy\"]!=\"A\"].shape[0]\n",
    "            \n",
    "            Favorite_A_for_correct = df2.loc[df2[\"Fav Strategy\"]==\"A\"][\"Fav Acc\"].sum()\n",
    "            Favorite_A_against_correct = df2.loc[df2[\"Fav Strategy\"]!=\"A\"][\"Fav Acc\"].sum()\n",
    "            \n",
    "            Favorite_for = Favorite_H_for + Favorite_A_for\n",
    "            Favorite_for_corr = Favorite_H_for_correct + Favorite_A_for_correct\n",
    "            Favorite_against = Favorite_H_against + Favorite_A_against\n",
    "            Favorite_against_corr = Favorite_H_against_correct + Favorite_A_against_correct\n",
    "        \n",
    "        elif model == \"HP\": \n",
    "            HP_H_for = df1.loc[df1[\"Pick Home\"]==\"H\"].shape[0]\n",
    "            HP_H_against = df1.loc[df1[\"Pick Home\"]!=\"H\"].shape[0]\n",
    "            \n",
    "            HP_H_for_correct = df1.loc[df1[\"Pick Home\"]==\"H\"][\"HP Acc\"].sum()\n",
    "            HP_H_against_correct = df1.loc[df1[\"Pick Home\"]!=\"H\"][\"HP Acc\"].sum()\n",
    "            \n",
    "            ###\n",
    "            HP_A_for = df2.loc[df2[\"Pick Home\"]==\"A\"].shape[0]\n",
    "            HP_A_against = df2.loc[df2[\"Pick Home\"]!=\"A\"].shape[0]\n",
    "            \n",
    "            HP_A_for_correct = df2.loc[df2[\"Pick Home\"]==\"A\"][\"HP Acc\"].sum()\n",
    "            HP_A_against_correct = df2.loc[df2[\"Pick Home\"]!=\"A\"][\"HP Acc\"].sum()\n",
    "            \n",
    "            HP_for = HP_H_for + HP_A_for\n",
    "            HP_for_corr = HP_H_for_correct + HP_A_for_correct\n",
    "            HP_against = HP_H_against + HP_A_against\n",
    "            HP_against_corr = HP_H_against_correct + HP_A_against_correct\n",
    "        \n",
    "    data = [Team, T_for, T_for_corr, T_against, T_against_corr, S_for, S_for_corr, S_against, S_against_corr,\n",
    "            N_for, N_for_corr, N_against, N_against_corr,\n",
    "           Democracy_for,Democracy_for_corr,Democracy_against,Democracy_against_corr,\n",
    "            Unanimous_for, Unanimous_for_corr, Unanimous_against, Unanimous_against_corr,\n",
    "           Favorite_for, Favorite_for_corr, Favorite_against, Favorite_against_corr,\n",
    "           HP_for, HP_for_corr, HP_against, HP_against_corr]\n",
    "    \n",
    "    Data_Team_Bias.loc[i] = data\n",
    "    i += 1\n",
    "    \n",
    "Data_Team_Bias = Data_Team_Bias.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Focus only on teams that are actually included in the validation set for the hyperparameter tuning\n",
    "Val_set_Teams = list(set(list(Val_set[\"HomeTeam\"])+list(Val_set[\"AwayTeam\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tree\n",
    "def Tree_val_eval_for(Data_Team_Bias, Teams_list):\n",
    "    Tree_for_data = Data_Team_Bias.loc[:,[\"Team\",\"Tree for\",\"Tree for correct\"]]\n",
    "    Tree_for_data = Tree_for_data.loc[Tree_for_data[\"Team\"].isin(Teams_list)]\n",
    "    Tree_for_data[\"%\"] = np.nan\n",
    "    for i in range(Tree_for_data.shape[0]):\n",
    "        try:\n",
    "            Tree_for_data.loc[i,\"%\"] = round(Tree_for_data.loc[i,\"Tree for correct\"] / Tree_for_data.loc[i,\"Tree for\"] *100,2)\n",
    "        except:\n",
    "            Tree_for_data.loc[i,\"%\"] = np.nan\n",
    "            \n",
    "    return Tree_for_data.sort_values(\"%\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def Tree_val_eval_against(Data_Team_Bias,Teams_list):\n",
    "    Tree_ag_data = Data_Team_Bias.loc[:,[\"Team\",\"Tree against\",\"Tree against correct\"]]\n",
    "    Tree_ag_data = Tree_ag_data.loc[Tree_ag_data[\"Team\"].isin(Teams_list)]\n",
    "    Tree_ag_data[\"%\"] = np.nan\n",
    "    for i in range(Tree_ag_data.shape[0]):\n",
    "        try:\n",
    "            Tree_ag_data.loc[i,\"%\"] = round(Tree_ag_data.loc[i,\"Tree against correct\"] / Tree_ag_data.loc[i,\"Tree against\"] *100,2)\n",
    "        except:\n",
    "            Tree_ag_data.loc[i,\"%\"] = np.nan\n",
    "            \n",
    "    return Tree_ag_data.sort_values(\"%\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM\n",
    "def SVM_val_eval_for(Data_Team_Bias,Teams_list):\n",
    "    SVM_for_data = Data_Team_Bias.loc[:,[\"Team\",\"SVM for\",\"SVM for correct\"]]\n",
    "    SVM_for_data = SVM_for_data.loc[SVM_for_data[\"Team\"].isin(Teams_list)]\n",
    "    SVM_for_data[\"%\"] = np.nan\n",
    "    for i in range(SVM_for_data.shape[0]):\n",
    "        try:\n",
    "            SVM_for_data.loc[i,\"%\"] = round(SVM_for_data.loc[i,\"SVM for correct\"] / SVM_for_data.loc[i,\"SVM for\"] *100,2)\n",
    "        except:\n",
    "            SVM_for_data.loc[i,\"%\"] = np.nan\n",
    "    \n",
    "    return SVM_for_data.sort_values(\"%\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def SVM_val_eval_against(Data_Team_Bias,Teams_list):\n",
    "    SVM_ag_data = Data_Team_Bias.loc[:,[\"Team\",\"SVM against\",\"SVM against correct\"]]\n",
    "    SVM_ag_data = SVM_ag_data.loc[SVM_ag_data[\"Team\"].isin(Teams_list)]\n",
    "    SVM_ag_data[\"%\"] = np.nan\n",
    "    for i in range(SVM_ag_data.shape[0]):\n",
    "        try:\n",
    "            SVM_ag_data.loc[i,\"%\"] = round(SVM_ag_data.loc[i,\"SVM against correct\"] / SVM_ag_data.loc[i,\"SVM against\"] *100,2)\n",
    "        except:\n",
    "            SVM_ag_data.loc[i,\"%\"] = np.nan\n",
    "            \n",
    "    return SVM_ag_data.sort_values(\"%\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN\n",
    "def NN_val_eval_for(Data_Team_Bias,Teams_list):\n",
    "    NN_for_data = Data_Team_Bias.loc[:,[\"Team\",\"NN for\",\"NN for correct\"]]\n",
    "    NN_for_data = NN_for_data.loc[NN_for_data[\"Team\"].isin(Teams_list)]\n",
    "    NN_for_data[\"%\"] = np.nan\n",
    "    for i in range(NN_for_data.shape[0]):\n",
    "        try:\n",
    "            NN_for_data.loc[i,\"%\"] = round(NN_for_data.loc[i,\"NN for correct\"] / NN_for_data.loc[i,\"NN for\"] *100,2)\n",
    "        except:\n",
    "            NN_for_data.loc[i,\"%\"] = np.nan\n",
    "            \n",
    "    return NN_for_data.sort_values(\"%\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def NN_val_eval_against(Data_Team_Bias,Teams_list):\n",
    "    NN_ag_data = Data_Team_Bias.loc[:,[\"Team\",\"NN against\",\"NN against correct\"]]\n",
    "    NN_ag_data = NN_ag_data.loc[NN_ag_data[\"Team\"].isin(Teams_list)]\n",
    "    NN_ag_data[\"%\"] = np.nan\n",
    "    for i in range(NN_ag_data.shape[0]):\n",
    "        try:\n",
    "            NN_ag_data.loc[i,\"%\"] = round(NN_ag_data.loc[i,\"NN against correct\"] / NN_ag_data.loc[i,\"NN against\"] *100,2)\n",
    "        except:\n",
    "            NN_ag_data.loc[i,\"%\"] = np.nan\n",
    "    \n",
    "    return NN_ag_data.sort_values(\"%\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Democracy\n",
    "def Democracy_val_eval_for(Data_Team_Bias,Teams_list):\n",
    "    Democracy_for_data = Data_Team_Bias.loc[:,[\"Team\",\"Democracy for\",\"Democracy for correct\"]]\n",
    "    Democracy_for_data = Democracy_for_data.loc[Democracy_for_data[\"Team\"].isin(Teams_list)]\n",
    "    Democracy_for_data[\"%\"] = np.nan\n",
    "    for i in range(Democracy_for_data.shape[0]):\n",
    "        try:\n",
    "            Democracy_for_data.loc[i,\"%\"] = round(Democracy_for_data.loc[i,\"Democracy for correct\"] / Democracy_for_data.loc[i,\"Democracy for\"] *100,2)\n",
    "        except:\n",
    "            Democracy_for_data.loc[i,\"%\"] = np.nan\n",
    "            \n",
    "    return Democracy_for_data.sort_values(\"%\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def Democracy_val_eval_against(Data_Team_Bias,Teams_list):\n",
    "    Democracy_ag_data = Data_Team_Bias.loc[:,[\"Team\",\"Democracy against\",\"Democracy against correct\"]]\n",
    "    Democracy_ag_data = Democracy_ag_data.loc[Democracy_ag_data[\"Team\"].isin(Teams_list)]\n",
    "    Democracy_ag_data[\"%\"] = np.nan\n",
    "    for i in range(Democracy_ag_data.shape[0]):\n",
    "        try:\n",
    "            Democracy_ag_data.loc[i,\"%\"] = round(Democracy_ag_data.loc[i,\"Democracy against correct\"] / Democracy_ag_data.loc[i,\"Democracy against\"] *100,2)\n",
    "        except:\n",
    "            Democracy_ag_data.loc[i,\"%\"] = np.nan\n",
    "    \n",
    "    return Democracy_ag_data.sort_values(\"%\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unanimous\n",
    "def Unanimous_val_eval_for(Data_Team_Bias,Teams_list):\n",
    "    Unanimous_for_data = Data_Team_Bias.loc[:,[\"Team\",\"Unanimous for\",\"Unanimous for correct\"]]\n",
    "    Unanimous_for_data = Unanimous_for_data.loc[Unanimous_for_data[\"Team\"].isin(Teams_list)]\n",
    "    Unanimous_for_data[\"%\"] = np.nan\n",
    "    for i in range(Unanimous_for_data.shape[0]):\n",
    "        try:\n",
    "            Unanimous_for_data.loc[i,\"%\"] = round(Unanimous_for_data.loc[i,\"Unanimous for correct\"] / Unanimous_for_data.loc[i,\"Unanimous for\"] *100,2)\n",
    "        except:\n",
    "            Unanimous_for_data.loc[i,\"%\"] = np.nan\n",
    "            \n",
    "    return Unanimous_for_data.sort_values(\"%\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def Unanimous_val_eval_against(Data_Team_Bias,Teams_list):\n",
    "    Unanimous_ag_data = Data_Team_Bias.loc[:,[\"Team\",\"Unanimous against\",\"Unanimous against correct\"]]\n",
    "    Unanimous_ag_data = Unanimous_ag_data.loc[Unanimous_ag_data[\"Team\"].isin(Teams_list)]\n",
    "    Unanimous_ag_data[\"%\"] = np.nan\n",
    "    for i in range(Unanimous_ag_data.shape[0]):\n",
    "        try:\n",
    "            Unanimous_ag_data.loc[i,\"%\"] = round(Unanimous_ag_data.loc[i,\"Unanimous against correct\"] / Unanimous_ag_data.loc[i,\"Unanimous against\"] *100,2)\n",
    "        except:\n",
    "            Unanimous_ag_data.loc[i,\"%\"] = np.nan\n",
    "    \n",
    "    return Unanimous_ag_data.sort_values(\"%\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Favorite\n",
    "def Favorite_val_eval_for(Data_Team_Bias,Teams_list):\n",
    "    Favorite_for_data = Data_Team_Bias.loc[:,[\"Team\",\"Favorite for\",\"Favorite for correct\"]]\n",
    "    Favorite_for_data = Favorite_for_data.loc[Favorite_for_data[\"Team\"].isin(Teams_list)]\n",
    "    Favorite_for_data[\"%\"] = np.nan\n",
    "    for i in range(Favorite_for_data.shape[0]):\n",
    "        try:\n",
    "            Favorite_for_data.loc[i,\"%\"] = round(Favorite_for_data.loc[i,\"Favorite for correct\"] / Favorite_for_data.loc[i,\"Favorite for\"] *100,2)\n",
    "        except:\n",
    "            Favorite_for_data.loc[i,\"%\"] = np.nan\n",
    "            \n",
    "    return Favorite_for_data.sort_values(\"%\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def Favorite_val_eval_against(Data_Team_Bias,Teams_list):\n",
    "    Favorite_ag_data = Data_Team_Bias.loc[:,[\"Team\",\"Favorite against\",\"Favorite against correct\"]]\n",
    "    Favorite_ag_data = Favorite_ag_data.loc[Favorite_ag_data[\"Team\"].isin(Teams_list)]\n",
    "    Favorite_ag_data[\"%\"] = np.nan\n",
    "    for i in range(Favorite_ag_data.shape[0]):\n",
    "        try:\n",
    "            Favorite_ag_data.loc[i,\"%\"] = round(Favorite_ag_data.loc[i,\"Favorite against correct\"] / Favorite_ag_data.loc[i,\"Favorite against\"] *100,2)\n",
    "        except:\n",
    "            Favorite_ag_data.loc[i,\"%\"] = np.nan\n",
    "    \n",
    "    return Favorite_ag_data.sort_values(\"%\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HP\n",
    "def HP_val_eval_for(Data_Team_Bias,Teams_list):\n",
    "    HP_for_data = Data_Team_Bias.loc[:,[\"Team\",\"HP for\",\"HP for correct\"]]\n",
    "    HP_for_data = HP_for_data.loc[HP_for_data[\"Team\"].isin(Teams_list)]\n",
    "    HP_for_data[\"%\"] = np.nan\n",
    "    for i in range(HP_for_data.shape[0]):\n",
    "        try:\n",
    "            HP_for_data.loc[i,\"%\"] = round(HP_for_data.loc[i,\"HP for correct\"] / HP_for_data.loc[i,\"HP for\"] *100,2)\n",
    "        except:\n",
    "            HP_for_data.loc[i,\"%\"] = np.nan\n",
    "            \n",
    "    return HP_for_data.sort_values(\"%\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def HP_val_eval_against(Data_Team_Bias,Teams_list):\n",
    "    HP_ag_data = Data_Team_Bias.loc[:,[\"Team\",\"HP against\",\"HP against correct\"]]\n",
    "    HP_ag_data = HP_ag_data.loc[HP_ag_data[\"Team\"].isin(Teams_list)]\n",
    "    HP_ag_data[\"%\"] = np.nan\n",
    "    for i in range(HP_ag_data.shape[0]):\n",
    "        try:\n",
    "            HP_ag_data.loc[i,\"%\"] = round(HP_ag_data.loc[i,\"HP against correct\"] / HP_ag_data.loc[i,\"HP against\"] *100,2)\n",
    "        except:\n",
    "            HP_ag_data.loc[i,\"%\"] = np.nan\n",
    "    \n",
    "    return HP_ag_data.sort_values(\"%\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal number of Teams included for picking strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_teams_optimizer(Data_Team_Bias, Validation_evaluation,Teams_list, Model, a, b):\n",
    "    \n",
    "    if Model == \"Tree\":\n",
    "        predicts_well_for = list(set(Tree_val_eval_for(Data_Team_Bias,Teams_list)[\"Team\"]))[:a]\n",
    "        predicts_well_against = list(set(Tree_val_eval_against(Data_Team_Bias,Teams_list)[\"Team\"]))[:b]\n",
    "    \n",
    "        data = Validation_evaluation[[\"Home Team\",\"Away Team\", \"Actual\", \"Tree Prediction\",\"Odds H\",\"Odds D\",\"Odds A\"]]\n",
    "        df1 = data.loc[(data[\"Home Team\"].isin(predicts_well_for))&(data[\"Tree Prediction\"] == \"H\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_for))&(data[\"Tree Prediction\"] == \"A\") |\n",
    "                      (data[\"Home Team\"].isin(predicts_well_against))&(data[\"Tree Prediction\"] == \"A\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_against))&(data[\"Tree Prediction\"] == \"H\")].reset_index(drop=True)\n",
    "        \n",
    "        df1[\"Correct\"] = np.nan\n",
    "                                     \n",
    "        for i in range(df1.shape[0]):\n",
    "            df1.loc[i, \"Correct\"] = np.where(df1.loc[i, \"Actual\"] == df1.loc[i, \"Tree Prediction\"],1,0)\n",
    "                                     \n",
    "        score = df1[\"Correct\"].sum()\n",
    "        Number_of_picks = df1.shape[0]\n",
    "    \n",
    "    elif Model == \"SVM\":\n",
    "        predicts_well_for = list(set(SVM_val_eval_for(Data_Team_Bias,Teams_list)[\"Team\"]))[:a]\n",
    "        predicts_well_against = list(set(SVM_val_eval_against(Data_Team_Bias,Teams_list)[\"Team\"]))[:b]\n",
    "    \n",
    "        data = Validation_evaluation[[\"Home Team\",\"Away Team\", \"Actual\", \"SVM Prediction\",\"Odds H\",\"Odds D\",\"Odds A\"]]\n",
    "        df1 = data.loc[(data[\"Home Team\"].isin(predicts_well_for))&(data[\"SVM Prediction\"] == \"H\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_for))&(data[\"SVM Prediction\"] == \"A\") |\n",
    "                      (data[\"Home Team\"].isin(predicts_well_against))&(data[\"SVM Prediction\"] == \"A\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_against))&(data[\"SVM Prediction\"] == \"H\")].reset_index(drop=True)\n",
    "        \n",
    "        df1[\"Correct\"] = np.nan\n",
    "                                     \n",
    "        for i in range(df1.shape[0]):\n",
    "            df1.loc[i, \"Correct\"] = np.where(df1.loc[i, \"Actual\"] == df1.loc[i, \"SVM Prediction\"],1,0)\n",
    "                                     \n",
    "        score = df1[\"Correct\"].sum()\n",
    "        Number_of_picks = df1.shape[0]                                 \n",
    "    \n",
    "    elif Model == \"NN\":\n",
    "        predicts_well_for = list(set(NN_val_eval_for(Data_Team_Bias,Teams_list)[\"Team\"]))[:a]\n",
    "        predicts_well_against = list(set(NN_val_eval_against(Data_Team_Bias,Teams_list)[\"Team\"]))[:b]\n",
    "    \n",
    "        data = Validation_evaluation[[\"Home Team\",\"Away Team\", \"Actual\", \"NN Prediction\",\"Odds H\",\"Odds D\",\"Odds A\"]]\n",
    "        df1 = data.loc[(data[\"Home Team\"].isin(predicts_well_for))&(data[\"NN Prediction\"] == \"H\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_for))&(data[\"NN Prediction\"] == \"A\") |\n",
    "                      (data[\"Home Team\"].isin(predicts_well_against))&(data[\"NN Prediction\"] == \"A\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_against))&(data[\"NN Prediction\"] == \"H\")].reset_index(drop=True)\n",
    "        \n",
    "        df1[\"Correct\"] = np.nan\n",
    "                                     \n",
    "        for i in range(df1.shape[0]):\n",
    "            df1.loc[i, \"Correct\"] = np.where(df1.loc[i, \"Actual\"] == df1.loc[i, \"NN Prediction\"],1,0)\n",
    "                                     \n",
    "        score = df1[\"Correct\"].sum()\n",
    "        Number_of_picks = df1.shape[0]                                 \n",
    "    \n",
    "    elif Model == \"Democracy\":\n",
    "        predicts_well_for = list(set(Democracy_val_eval_for(Data_Team_Bias,Teams_list)[\"Team\"]))[:a]\n",
    "        predicts_well_against = list(set(Democracy_val_eval_against(Data_Team_Bias,Teams_list)[\"Team\"]))[:b]\n",
    "    \n",
    "        data = Validation_evaluation[[\"Home Team\",\"Away Team\", \"Actual\", \"Democracy\",\"Odds H\",\"Odds D\",\"Odds A\"]]\n",
    "        df1 = data.loc[(data[\"Home Team\"].isin(predicts_well_for))&(data[\"Democracy\"] == \"H\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_for))&(data[\"Democracy\"] == \"A\") |\n",
    "                      (data[\"Home Team\"].isin(predicts_well_against))&(data[\"Democracy\"] == \"A\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_against))&(data[\"Democracy\"] == \"H\")].reset_index(drop=True)\n",
    "        \n",
    "        df1[\"Correct\"] = np.nan\n",
    "                                     \n",
    "        for i in range(df1.shape[0]):\n",
    "            df1.loc[i, \"Correct\"] = np.where(df1.loc[i, \"Actual\"] == df1.loc[i, \"Democracy\"],1,0)\n",
    "                                     \n",
    "        score = df1[\"Correct\"].sum()\n",
    "        Number_of_picks = df1.shape[0] \n",
    "    \n",
    "    elif Model == \"Favorite\":\n",
    "        predicts_well_for = list(set(Favorite_val_eval_for(Data_Team_Bias,Teams_list)[\"Team\"]))[:a]\n",
    "        predicts_well_against = list(set(Favorite_val_eval_against(Data_Team_Bias,Teams_list)[\"Team\"]))[:b]\n",
    "    \n",
    "        data = Validation_evaluation[[\"Home Team\",\"Away Team\", \"Actual\", \"Fav Strategy\",\"Odds H\",\"Odds D\",\"Odds A\"]]\n",
    "        df1 = data.loc[(data[\"Home Team\"].isin(predicts_well_for))&(data[\"Fav Strategy\"] == \"H\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_for))&(data[\"Fav Strategy\"] == \"A\") |\n",
    "                      (data[\"Home Team\"].isin(predicts_well_against))&(data[\"Fav Strategy\"] == \"A\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_against))&(data[\"Fav Strategy\"] == \"H\")].reset_index(drop=True)\n",
    "        \n",
    "        df1[\"Correct\"] = np.nan\n",
    "                                     \n",
    "        for i in range(df1.shape[0]):\n",
    "            df1.loc[i, \"Correct\"] = np.where(df1.loc[i, \"Actual\"] == df1.loc[i, \"Fav Strategy\"],1,0)\n",
    "                                     \n",
    "        score = df1[\"Correct\"].sum()\n",
    "        Number_of_picks = df1.shape[0] \n",
    "    \n",
    "    elif Model == \"Unanimous\":\n",
    "        predicts_well_for = list(set(Unanimous_val_eval_for(Data_Team_Bias,Teams_list)[\"Team\"]))[:a]\n",
    "        predicts_well_against = list(set(Unanimous_val_eval_against(Data_Team_Bias,Teams_list)[\"Team\"]))[:b]\n",
    "    \n",
    "        data = Validation_evaluation[[\"Home Team\",\"Away Team\", \"Actual\", \"Unanimous\",\"Odds H\",\"Odds D\",\"Odds A\"]]\n",
    "        df1 = data.loc[(data[\"Home Team\"].isin(predicts_well_for))&(data[\"Unanimous\"] == \"H\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_for))&(data[\"Unanimous\"] == \"A\") |\n",
    "                      (data[\"Home Team\"].isin(predicts_well_against))&(data[\"Unanimous\"] == \"A\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_against))&(data[\"Unanimous\"] == \"H\")].reset_index(drop=True)\n",
    "        \n",
    "        df1[\"Correct\"] = np.nan\n",
    "                                     \n",
    "        for i in range(df1.shape[0]):\n",
    "            df1.loc[i, \"Correct\"] = np.where(df1.loc[i, \"Actual\"] == df1.loc[i, \"Unanimous\"],1,0)\n",
    "                                     \n",
    "        score = df1[\"Correct\"].sum()\n",
    "        Number_of_picks = df1.shape[0] \n",
    "    \n",
    "    elif Model == \"HP\":\n",
    "        predicts_well_for = list(set(HP_val_eval_for(Data_Team_Bias,Teams_list)[\"Team\"]))[:a]\n",
    "        predicts_well_against = list(set(HP_val_eval_against(Data_Team_Bias,Teams_list)[\"Team\"]))[:b]\n",
    "    \n",
    "        data = Validation_evaluation[[\"Home Team\",\"Away Team\", \"Actual\", \"Pick Home\",\"Odds H\",\"Odds D\",\"Odds A\"]]\n",
    "        df1 = data.loc[(data[\"Home Team\"].isin(predicts_well_for))&(data[\"Pick Home\"] == \"H\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_for))&(data[\"Pick Home\"] == \"A\") |\n",
    "                      (data[\"Home Team\"].isin(predicts_well_against))&(data[\"Pick Home\"] == \"A\") |\n",
    "                      (data[\"Away Team\"].isin(predicts_well_against))&(data[\"Pick Home\"] == \"H\")].reset_index(drop=True)\n",
    "        \n",
    "        df1[\"Correct\"] = np.nan\n",
    "                                     \n",
    "        for i in range(df1.shape[0]):\n",
    "            df1.loc[i, \"Correct\"] = np.where(df1.loc[i, \"Actual\"] == df1.loc[i, \"Pick Home\"],1,0)\n",
    "                                     \n",
    "        score = df1[\"Correct\"].sum()\n",
    "        Number_of_picks = df1.shape[0] \n",
    "    \n",
    "    \n",
    "    return score, Number_of_picks, df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_scoring = pd.DataFrame()\n",
    "    \n",
    "for Model in [\"Tree\",\"SVM\",\"NN\",\"Democracy\",\"Unanimous\",\"Favorite\",\"HP\"]:\n",
    "    for a in range(4,15):\n",
    "        for b in range(4,15):\n",
    "                \n",
    "            score, Number_of_picks, data = number_of_teams_optimizer(Data_Team_Bias, Validation_evaluation, Val_set_Teams, Model, a, b)\n",
    "            params = [[Model, score, Number_of_picks, a, b]]\n",
    "            Validation_scoring = Validation_scoring.append(params)\n",
    "            #print(Model, a, b)\n",
    "            \n",
    "Validation_scoring = Validation_scoring.reset_index(drop=True)\n",
    "Validation_scoring.columns = [\"Model\", \"score\",\"Number of picks\",\"Hyperparam 'for'\",\"Hyperparam 'against'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_scoring[\"%\"] = Validation_scoring[\"score\"] / Validation_scoring[\"Number of picks\"]\n",
    "Validation_scoring[\"Ranker\"] = Validation_scoring[\"score\"] * Validation_scoring[\"%\"] ** 2 # higher weight for a high percentage\n",
    "Validation_scoring.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ranked_val = Validation_scoring.sort_values(\"Ranker\", ascending=False)\n",
    "Ranked_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the test-set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Result_evaluation = pd.DataFrame(columns=[\"Actual\",\"Tree Prediction\",\"SVM Prediction\",\"NN Prediction\"])\n",
    "for i in range(Tree_test_target.shape[0]):\n",
    "    Result_evaluation.loc[i,\"Actual\"] = Tree_test_target[i]\n",
    "    Result_evaluation.loc[i,\"Tree Prediction\"] = Tree_test_prediction[i]\n",
    "    Result_evaluation.loc[i,\"SVM Prediction\"] = SVM_test_prediction[i]\n",
    "    Result_evaluation.loc[i,\"NN Prediction\"] = NN_test_Result_prediction.loc[i,\"NN Prediction\"]\n",
    "    \n",
    "Result_evaluation[\"Home Team\"] = Test_set[\"HomeTeam\"]\n",
    "Result_evaluation[\"Away Team\"] = Test_set[\"AwayTeam\"]\n",
    "Result_evaluation[\"Democracy\"] = np.nan\n",
    "Result_evaluation[\"Unanimous\"] = np.nan\n",
    "Result_evaluation[\"Fav Strategy\"] = np.nan\n",
    "\n",
    "for i in range(Result_evaluation.shape[0]):\n",
    "    \n",
    "    if (Result_evaluation.loc[i,\"Tree Prediction\"] == Result_evaluation.loc[i,\"SVM Prediction\"])&(Result_evaluation.loc[i,\"Tree Prediction\"] == Result_evaluation.loc[i,\"NN Prediction\"]):\n",
    "        Result_evaluation.loc[i,\"Unanimous\"] = Result_evaluation.loc[i,\"Tree Prediction\"]\n",
    "    \n",
    "    #\n",
    "    if (Result_evaluation.loc[i,\"Tree Prediction\"] == Result_evaluation.loc[i,\"SVM Prediction\"])&(Result_evaluation.loc[i,\"Tree Prediction\"] != Result_evaluation.loc[i,\"NN Prediction\"]):\n",
    "        Result_evaluation.loc[i,\"Democracy\"] = Result_evaluation.loc[i,\"Tree Prediction\"]\n",
    "    elif (Result_evaluation.loc[i,\"Tree Prediction\"] != Result_evaluation.loc[i,\"SVM Prediction\"])&(Result_evaluation.loc[i,\"Tree Prediction\"] == Result_evaluation.loc[i,\"NN Prediction\"]):\n",
    "        Result_evaluation.loc[i,\"Democracy\"] = Result_evaluation.loc[i,\"Tree Prediction\"]\n",
    "    elif (Result_evaluation.loc[i,\"Tree Prediction\"] != Result_evaluation.loc[i,\"SVM Prediction\"])&(Result_evaluation.loc[i,\"SVM Prediction\"] == Result_evaluation.loc[i,\"NN Prediction\"]):\n",
    "        Result_evaluation.loc[i,\"Democracy\"] = Result_evaluation.loc[i,\"SVM Prediction\"]\n",
    "    elif (Result_evaluation.loc[i,\"Tree Prediction\"] == Result_evaluation.loc[i,\"SVM Prediction\"])&(Result_evaluation.loc[i,\"Tree Prediction\"] == Result_evaluation.loc[i,\"NN Prediction\"]):\n",
    "        Result_evaluation.loc[i,\"Democracy\"] = Result_evaluation.loc[i,\"Tree Prediction\"]\n",
    "    #\n",
    "    Result_evaluation.loc[i,\"Fav Strategy\"] = np.where((Test_set.loc[i,\"Odds H\"] < Test_set.loc[i,\"Odds A\"]) & (Test_set.loc[i,\"Odds H\"] < Test_set.loc[i,\"Odds D\"]), \"H\", Result_evaluation.loc[i,\"Fav Strategy\"])\n",
    "    Result_evaluation.loc[i,\"Fav Strategy\"] = np.where((Test_set.loc[i,\"Odds A\"] < Test_set.loc[i,\"Odds D\"]) & (Test_set.loc[i,\"Odds A\"] < Test_set.loc[i,\"Odds D\"]), \"A\", Result_evaluation.loc[i,\"Fav Strategy\"])\n",
    "    Result_evaluation.loc[i,\"Fav Strategy\"] = np.where((Test_set.loc[i,\"Odds D\"] < Test_set.loc[i,\"Odds H\"]) & (Test_set.loc[i,\"Odds D\"] < Test_set.loc[i,\"Odds A\"]), \"D\", Result_evaluation.loc[i,\"Fav Strategy\"])\n",
    "    \n",
    "Result_evaluation = Result_evaluation[[\"Home Team\",\"Away Team\", \"Actual\", \"Tree Prediction\", \"SVM Prediction\",\"NN Prediction\",\"Democracy\", \"Unanimous\",\"Fav Strategy\"]]\n",
    "Result_evaluation[\"Pick Home\"] = \"H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_evaluation = pd.concat([Result_evaluation, Test_set[[\"Odds H\",\"Odds D\",\"Odds A\"]]], axis = 1)\n",
    "Result_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Test_set_Teams = list(set(list(Test_set[\"HomeTeam\"])+list(Test_set[\"AwayTeam\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_Tree, Number_of_picks_Tree, data_Tree = number_of_teams_optimizer(Data_Team_Bias, Result_evaluation, Test_set_Teams, \"Tree\", Ranked_val.loc[Ranked_val[\"Model\"] == \"Tree\"].reset_index(drop=True).loc[0,\"Hyperparam 'for'\"], Ranked_val.loc[Ranked_val[\"Model\"] == \"Tree\"].reset_index(drop=True).loc[0,\"Hyperparam 'against'\"])\n",
    "score_SVM, Number_of_picks_SVM, data_SVM = number_of_teams_optimizer(Data_Team_Bias, Result_evaluation, Test_set_Teams, \"SVM\", Ranked_val.loc[Ranked_val[\"Model\"] == \"SVM\"].reset_index(drop=True).loc[0,\"Hyperparam 'for'\"], Ranked_val.loc[Ranked_val[\"Model\"] == \"SVM\"].reset_index(drop=True).loc[0,\"Hyperparam 'against'\"])\n",
    "score_NN, Number_of_picks_NN, data_NN = number_of_teams_optimizer(Data_Team_Bias, Result_evaluation, Test_set_Teams, \"NN\", Ranked_val.loc[Ranked_val[\"Model\"] == \"NN\"].reset_index(drop=True).loc[0,\"Hyperparam 'for'\"], Ranked_val.loc[Ranked_val[\"Model\"] == \"NN\"].reset_index(drop=True).loc[0,\"Hyperparam 'against'\"])\n",
    "\n",
    "score_D, Number_of_picks_D, data_D = number_of_teams_optimizer(Data_Team_Bias, Result_evaluation, Test_set_Teams, \"Democracy\", Ranked_val.loc[Ranked_val[\"Model\"] == \"Democracy\"].reset_index(drop=True).loc[0,\"Hyperparam 'for'\"], Ranked_val.loc[Ranked_val[\"Model\"] == \"NN\"].reset_index(drop=True).loc[0,\"Hyperparam 'against'\"])\n",
    "score_U, Number_of_picks_U, data_U = number_of_teams_optimizer(Data_Team_Bias, Result_evaluation, Test_set_Teams, \"Unanimous\", Ranked_val.loc[Ranked_val[\"Model\"] == \"Unanimous\"].reset_index(drop=True).loc[0,\"Hyperparam 'for'\"], Ranked_val.loc[Ranked_val[\"Model\"] == \"NN\"].reset_index(drop=True).loc[0,\"Hyperparam 'against'\"])\n",
    "score_F, Number_of_picks_F, data_F = number_of_teams_optimizer(Data_Team_Bias, Result_evaluation, Test_set_Teams, \"Favorite\", Ranked_val.loc[Ranked_val[\"Model\"] == \"Favorite\"].reset_index(drop=True).loc[0,\"Hyperparam 'for'\"], Ranked_val.loc[Ranked_val[\"Model\"] == \"NN\"].reset_index(drop=True).loc[0,\"Hyperparam 'against'\"])\n",
    "score_H, Number_of_picks_H, data_H = number_of_teams_optimizer(Data_Team_Bias, Result_evaluation, Test_set_Teams, \"HP\", Ranked_val.loc[Ranked_val[\"Model\"] == \"HP\"].reset_index(drop=True).loc[0,\"Hyperparam 'for'\"], Ranked_val.loc[Ranked_val[\"Model\"] == \"NN\"].reset_index(drop=True).loc[0,\"Hyperparam 'against'\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_Tree, Number_of_picks_Tree, score_Tree/Number_of_picks_Tree)\n",
    "print(score_SVM, Number_of_picks_SVM, score_SVM/Number_of_picks_SVM)\n",
    "print(score_NN, Number_of_picks_NN, score_NN/ Number_of_picks_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_D, Number_of_picks_D, score_D/Number_of_picks_D)\n",
    "print(score_U, Number_of_picks_U, score_U / Number_of_picks_U)\n",
    "print(score_F, Number_of_picks_F, score_F/Number_of_picks_F)\n",
    "print(score_H, Number_of_picks_H, score_H/Number_of_picks_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the data to evaluate in another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General predictions\n",
    "Result_evaluation.to_csv(r'C:\\Users\\### LOCAL PATH ###\\Result_evaluation.txt', header = True, index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting 2018/19 season so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import - cleaned 2018/19 data from the ETL notebook\n",
    "Data_current_season = pd.read_csv(r\"C:\\Users\\### LOCAL PATH ###\\data_current_season.txt\", sep = \"\\t\")\n",
    "Data_current_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Teams_list = list(set(list(set(Data_current_season[\"HomeTeam\"]))+list(set(Data_current_season[\"AwayTeam\"]))))\n",
    "Seasons_list = list(Data_current_season[\"Season start\"].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the data\n",
    "# NN\n",
    "\n",
    "# adding the prev season's data\n",
    "Season_data_current = pd.DataFrame()\n",
    "\n",
    "for Team in Teams_list:\n",
    "    data1 = Data_current_season.loc[Data_current_season[\"HomeTeam\"] == Team]\n",
    "    data2 = Data_current_season.loc[Data_current_season[\"AwayTeam\"] == Team]\n",
    "    \n",
    "    for year in Seasons_list:\n",
    "        \n",
    "        data1_b = data1.loc[data1[\"Season start\"] == year]\n",
    "        data2_b = data2.loc[data2[\"Season start\"] == year]\n",
    "\n",
    "        Goals_H_for = data1_b[\"Home Goals\"].sum()\n",
    "        Goals_H_against = data1_b[\"Away Goals\"].sum()\n",
    "        Goals_A_for = data2_b[\"Away Goals\"].sum()\n",
    "        Goals_A_against = data2_b[\"Home Goals\"].sum()\n",
    "\n",
    "        # \"Counting\" Wins, draws, losses - first at home then away\n",
    "        Wins_H = data1_b[\"Result\"].str.count(\"H\").sum()\n",
    "        Ties_H = data1_b[\"Result\"].str.count(\"D\").sum()\n",
    "        Losses_H = data1_b[\"Result\"].str.count(\"A\").sum()\n",
    "        \n",
    "        Wins_A = data2_b[\"Result\"].str.count(\"A\").sum()\n",
    "        Ties_A = data2_b[\"Result\"].str.count(\"D\").sum()\n",
    "        Losses_A = data2_b[\"Result\"].str.count(\"H\").sum()\n",
    "        \n",
    "        Total_points = Wins_H * 3 + Ties_H + Wins_A * 3 + Ties_A\n",
    "              \n",
    "        Season_data_current = Season_data_current.append([[Team, year, Total_points, Wins_H, Ties_H, Losses_H, Goals_H_for, Goals_H_against, Wins_A, Ties_A, Losses_A, Goals_A_for, Goals_A_against]])\n",
    "\n",
    "Season_data_current.columns = [\"Team\",\"Season start\",\"Total Points\",\"Home W\",\"Home T\",\"Home L\",\"Home Goals For\", \"Home Goals Against\",\"Away W\",\"Away T\",\"Away L\",\"Away Goals For\", \"Away Goals Against\"]\n",
    "Season_data_current.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#\n",
    "# Combining the data - prev season summary for each Game\n",
    "Season_data_current[\"prev season\"] = pd.to_numeric(Season_data_current[\"Season start\"]) +1\n",
    "Season_data_current = Season_data_current.drop(\"Season start\", axis=1)\n",
    "# To merge\n",
    "Season_data_current2 = Season_data_current.rename(columns={\"Team\":\"AwayTeam\",\"Total Points\":\"T2 Total Points\",\"Home W\":\"T2 Home W\",\"Home T\":\"T2 Home T\",\"Home L\":\"T2 Home L\", \"Home Goals For\":\"T2 Home Goals For\",\"Home Goals Against\":\"T2 Home Goals Against\",\"Away W\":\"T2 Away W\",\"Away T\":\"T2 Away T\",\"Away L\":\"T2 Away L\",\"Away Goals For\":\"T2 Away Goals For\",\"Away Goals Against\":\"T2 Away Goals Against\"})\n",
    "Season_data_current = Season_data_current.rename(columns={\"Team\":\"HomeTeam\",\"Total Points\":\"T1 Total Points\",\"Home W\":\"T1 Home W\",\"Home T\":\"T1 Home T\",\"Home L\":\"T1 Home L\", \"Home Goals For\":\"T1 Home Goals For\",\"Home Goals Against\":\"T1 Home Goals Against\",\"Away W\":\"T1 Away W\",\"Away T\":\"T1 Away T\",\"Away L\":\"T1 Away L\",\"Away Goals For\":\"T1 Away Goals For\",\"Away Goals Against\":\"T1 Away Goals Against\"})\n",
    "\n",
    "#\n",
    "Data_current_season[\"Season start\"] = pd.to_numeric(Data_current_season[\"Season start\"])\n",
    "Data_current_season[\"prev season\"] = pd.to_numeric(Data_current_season[\"Season start\"])\n",
    "\n",
    "#\n",
    "# merging season data\n",
    "Data_current_season = Data_current_season.merge(Season_data_current, how=\"left\",on=[\"prev season\",\"HomeTeam\"])\n",
    "Data_current_season = Data_current_season.merge(Season_data_current2, how=\"left\", on=[\"prev season\",\"AwayTeam\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Model data prep - NN\n",
    "Dataset_step1, Indicator1 = prev_season_weight(NN_weight, Data_current_season)\n",
    "Prev_Games_avg_data = prev_games_avg_data(NN_n, Indicator1, Dataset_step1)\n",
    "Prev_Games_avg_data_for_rank, Indicator2 = prev_games_avg_data_for_rank(NN_m, Indicator1, Dataset_step1)\n",
    "Prev_Games_ranking = prev_games_ranking(Indicator2, Prev_Games_avg_data_for_rank)              \n",
    "Data_for_model = data_merge(Prev_Games_avg_data, Prev_Games_avg_data_for_rank, Prev_Games_ranking, Dataset_step1).dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####\n",
    "# Curent Season NN\n",
    "Current_season_Target = Data_for_model.loc[:, \"Result\"]\n",
    "Current_season_Features = Data_for_model.iloc[:, 7:].values\n",
    "\n",
    "encoded_Y3 = encoder.transform(Current_season_Target)\n",
    "NN_Current_season_target = np_utils.to_categorical(encoded_Y3)\n",
    "    \n",
    "# scaling\n",
    "\n",
    "if NN_Scaler == 2:\n",
    "    Current_season_Features = scaleData(Current_season_Features)\n",
    "elif NN_Scaler == 1:\n",
    "    Current_season_Features = scaleData2(Current_season_Features)\n",
    "    \n",
    "else:\n",
    "    for i in range(Current_season_Features.shape[0]):\n",
    "        for j in range(Current_season_Features.shape[1]):\n",
    "            Current_season_Features[i,j] = 2 * (Current_season_Features[i,j] - Current_season_Features[:,j].min()) / (Current_season_Features[:,j].max() - Current_season_Features[:,j].min())-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## SVM\n",
    "Dataset_step1, Indicator1 = prev_season_weight(SVM_weight, Data_current_season)\n",
    "Prev_Games_avg_data = prev_games_avg_data(SVM_n, Indicator1, Dataset_step1)\n",
    "Prev_Games_avg_data_for_rank, Indicator2 = prev_games_avg_data_for_rank(SVM_m, Indicator1, Dataset_step1)\n",
    "Prev_Games_ranking = prev_games_ranking(Indicator2, Prev_Games_avg_data_for_rank)              \n",
    "Data_for_model_SVM = data_merge(Prev_Games_avg_data, Prev_Games_avg_data_for_rank, Prev_Games_ranking, Dataset_step1).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## SVM\n",
    "#\n",
    "Current_season_Features_SVM = Data_for_model_SVM.iloc[:, 7:].values\n",
    "\n",
    "\n",
    "# scaling\n",
    "if SVM_Scaler == 2:\n",
    "    Current_season_Features_SVM = scaleData(Current_season_Features_SVM)\n",
    "\n",
    "elif SVM_Scaler == 1:\n",
    "    Current_season_Features_SVM = scaleData2(Current_season_Features_SVM)\n",
    "    \n",
    "else:\n",
    "    for i in range(Current_season_Features_SVM.shape[0]):\n",
    "        for j in range(Current_season_Features_SVM.shape[1]):\n",
    "            Current_season_Features_SVM[i,j] = 2 * (Current_season_Features_SVM[i,j] - Current_season_Features_SVM[:,j].min()) / (Current_season_Features_SVM[:,j].max() - Current_season_Features_SVM[:,j].min())-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tree\n",
    "Dataset_step1, Indicator1 = prev_season_weight(Tree_weight, Data_current_season)\n",
    "Prev_Games_avg_data = prev_games_avg_data(Tree_n, Indicator1, Dataset_step1)\n",
    "Prev_Games_avg_data_for_rank, Indicator2 = prev_games_avg_data_for_rank(Tree_m, Indicator1, Dataset_step1)\n",
    "Prev_Games_ranking = prev_games_ranking(Indicator2, Prev_Games_avg_data_for_rank)              \n",
    "Data_for_model_fit_tree = data_merge(Prev_Games_avg_data, Prev_Games_avg_data_for_rank, Prev_Games_ranking, Dataset_step1).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tree\n",
    "# Getting dummies from the trained data\n",
    "\n",
    "Current_season_Features_tree = Data_for_model_fit_tree.iloc[:, 7:].values\n",
    "\n",
    "# scaling\n",
    "if Tree_Scaler == 2:\n",
    "    Current_season_Features_tree = scaleData(Current_season_Features_tree)\n",
    "\n",
    "elif Tree_Scaler == 1:\n",
    "    Current_season_Features_tree = scaleData2(Current_season_Features_tree)\n",
    "    \n",
    "\n",
    "else:\n",
    "    if Tree_Team_Dummy == 0:\n",
    "        for i in range(Current_season_Features_tree.shape[0]-helper):\n",
    "            for j in range(Current_season_Features_tree.shape[1]):\n",
    "                Current_season_Features_tree[i,j] = 2 * (Current_season_Features_tree[i,j] - Current_season_Features_tree[:,j].min()) / (Current_season_Features_tree[:,j].max() - Current_season_Features_tree[:,j].min())-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.evaluate(Current_season_Features, NN_Current_season_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_season_prediction = NN_model.predict(Current_season_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_Current_season_Result_prediction = pd.DataFrame(columns=[\"NN Prediction\"])\n",
    "\n",
    "for i in range(NN_Current_season_target.shape[0]):\n",
    "    if (Current_season_prediction[i,0] > Current_season_prediction[i,1]) & (Current_season_prediction[i,0] >Current_season_prediction[i,2]):\n",
    "        NN_Current_season_Result_prediction.loc[i,\"NN Prediction\"] = \"A\"\n",
    "        \n",
    "    elif (Current_season_prediction[i,1] > Current_season_prediction[i,0]) & (Current_season_prediction[i,1] >Current_season_prediction[i,2]):\n",
    "        NN_Current_season_Result_prediction.loc[i,\"NN Prediction\"] = \"D\"    \n",
    "    \n",
    "    elif (Current_season_prediction[i,2] > Current_season_prediction[i,0]) & (Current_season_prediction[i,2] >Current_season_prediction[i,1]):\n",
    "        NN_Current_season_Result_prediction.loc[i,\"NN Prediction\"] = \"H\"\n",
    "        \n",
    "    else:\n",
    "        NN_Current_season_Result_prediction.loc[i,\"NN Prediction\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(Current_season_Target, SVM_clf.predict(Current_season_Features_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_prediction = SVM_clf.predict(Current_season_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(Current_season_Target, Result_Tree.predict(Current_season_Features_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree_prediction = Result_Tree.predict(Current_season_Features_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_season = Data_for_model.loc[:,[\"Season start\",\"Gameday\",\"HomeTeam\",\"AwayTeam\",\"Result\",\"Odds H\",\"Odds D\",\"Odds A\"]]\n",
    "                  \n",
    "Current_season[\"Tree Prediction\"] =np.nan\n",
    "Current_season[\"SVM Prediction\"] =np.nan\n",
    "Current_season[\"NN Prediction\"] =np.nan\n",
    "                  \n",
    "for i in range(Current_season.shape[0]):\n",
    "    \n",
    "    Current_season.loc[i,\"Tree Prediction\"] = Tree_prediction[i]\n",
    "    Current_season.loc[i,\"SVM Prediction\"] = SVM_prediction[i]\n",
    "    Current_season.loc[i,\"NN Prediction\"] = NN_Current_season_Result_prediction.loc[i,\"NN Prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_season[\"Democracy\"] =np.nan\n",
    "Current_season[\"Unanimous\"] =np.nan\n",
    "Current_season[\"Fav Strategy\"] = np.nan\n",
    "Current_season[\"Home Pick\"] = \"H\"\n",
    "\n",
    "    \n",
    "for i in range(Current_season.shape[0]):\n",
    "    \n",
    "    if (Current_season.loc[i,\"Tree Prediction\"] == Current_season.loc[i,\"SVM Prediction\"])&(Current_season.loc[i,\"Tree Prediction\"] == Current_season.loc[i,\"NN Prediction\"]):\n",
    "        Current_season.loc[i,\"Unanimous\"] = Current_season.loc[i,\"Tree Prediction\"]\n",
    "    \n",
    "    #\n",
    "    if (Current_season.loc[i,\"Tree Prediction\"] == Current_season.loc[i,\"SVM Prediction\"])&(Current_season.loc[i,\"Tree Prediction\"] != Current_season.loc[i,\"NN Prediction\"]):\n",
    "        Current_season.loc[i,\"Democracy\"] = Current_season.loc[i,\"Tree Prediction\"]\n",
    "    elif (Current_season.loc[i,\"Tree Prediction\"] != Current_season.loc[i,\"SVM Prediction\"])&(Current_season.loc[i,\"Tree Prediction\"] == Current_season.loc[i,\"NN Prediction\"]):\n",
    "        Current_season.loc[i,\"Democracy\"] = Current_season.loc[i,\"Tree Prediction\"]\n",
    "    elif (Current_season.loc[i,\"Tree Prediction\"] != Current_season.loc[i,\"SVM Prediction\"])&(Current_season.loc[i,\"SVM Prediction\"] == Current_season.loc[i,\"NN Prediction\"]):\n",
    "        Current_season.loc[i,\"Democracy\"] = Current_season.loc[i,\"SVM Prediction\"]\n",
    "    elif (Current_season.loc[i,\"Tree Prediction\"] == Current_season.loc[i,\"SVM Prediction\"])&(Current_season.loc[i,\"Tree Prediction\"] == Current_season.loc[i,\"NN Prediction\"]):\n",
    "        Current_season.loc[i,\"Democracy\"] = Current_season.loc[i,\"Tree Prediction\"]\n",
    "    \n",
    "    \n",
    "    Current_season.loc[i,\"Fav Strategy\"] = np.where((Current_season.loc[i,\"Odds H\"] < Current_season.loc[i,\"Odds A\"]) & (Current_season.loc[i,\"Odds H\"] < Current_season.loc[i,\"Odds D\"]), \"H\", Current_season.loc[i,\"Fav Strategy\"])\n",
    "    Current_season.loc[i,\"Fav Strategy\"] = np.where((Current_season.loc[i,\"Odds A\"] < Current_season.loc[i,\"Odds D\"]) & (Current_season.loc[i,\"Odds A\"] < Current_season.loc[i,\"Odds D\"]), \"A\", Current_season.loc[i,\"Fav Strategy\"])\n",
    "    Current_season.loc[i,\"Fav Strategy\"] = np.where((Current_season.loc[i,\"Odds D\"] < Current_season.loc[i,\"Odds H\"]) & (Current_season.loc[i,\"Odds D\"] < Current_season.loc[i,\"Odds A\"]), \"D\", Current_season.loc[i,\"Fav Strategy\"])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = pd.DataFrame(columns=[\"Tree Prediction\",\"SVM Prediction\",\"NN Prediction\",\"Democracy\",\"Unanimous\",\"Fav Strategy\",\"Home Pick\"])\n",
    "\n",
    "for i in range(Current_season.shape[0]):\n",
    "    for column in Accuracy.columns:\n",
    "        if str(Current_season.loc[i,column]) == \"nan\":\n",
    "            Accuracy.loc[i,column] = np.nan\n",
    "        \n",
    "        else:\n",
    "            Accuracy.loc[i,column] = np.where(Current_season.loc[i,\"Result\"] == Current_season.loc[i,column],1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Current_Accuracy = pd.concat([Current_season[[\"Season start\",\"Gameday\",\"HomeTeam\",\"AwayTeam\",\"Result\"]], Accuracy], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_Accuracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_Accuracy_correct = Current_Accuracy.groupby(\"Gameday\", as_index=False)[\"Tree Prediction\",\"SVM Prediction\",\"NN Prediction\",\"Democracy\",\"Unanimous\",\"Fav Strategy\",\"Home Pick\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_Accuracy_count = Current_Accuracy.groupby(\"Gameday\", as_index=False)[\"Tree Prediction\",\"SVM Prediction\",\"NN Prediction\",\"Democracy\",\"Unanimous\",\"Fav Strategy\",\"Home Pick\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Current_Accuracy_correct.shape[0]):\n",
    "    for j in range(1,Current_Accuracy_correct.shape[1]):\n",
    "        Current_Accuracy_correct.iloc[i,j] = round(Current_Accuracy_correct.iloc[i,j] / Current_Accuracy_count.iloc[i,j],2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_Accuracy_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_Accuracy_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current season accuracy by approach: \\n \\n\" ,\n",
    "      '{:10}'.format(\"Tree: \")+ '{:>13}'.format(str(round((Current_Accuracy[\"Tree Prediction\"].sum() / Current_Accuracy[\"Tree Prediction\"].dropna().shape[0])*100,2))+\"% \\n\"), \n",
    "      '{:10}'.format(\"SVM: \")+'{:>13}'.format(str(round((Current_Accuracy[\"SVM Prediction\"].sum()/ Current_Accuracy[\"SVM Prediction\"].dropna().shape[0])*100,2))+\"% \\n\"),\n",
    "     '{:10}'.format(\"NN: \")+ '{:>13}'.format(str(round((Current_Accuracy[\"NN Prediction\"].sum()/ Current_Accuracy[\"NN Prediction\"].dropna().shape[0])*100,2))+\"% \\n\"),\n",
    "    '{:10}'.format(\"Democracy: \")+'{:>12}'.format(str(round((Current_Accuracy[\"Democracy\"].sum()/ Current_Accuracy[\"Democracy\"].dropna().shape[0])*100,2))+\"% \\n\"),\n",
    "     '{:10}'.format(\"Unanimous: \")+ '{:>12}'.format(str(round((Current_Accuracy[\"Unanimous\"].sum()/ Current_Accuracy[\"Unanimous\"].dropna().shape[0])*100,2))+\"% \\n\"),\n",
    "      '{:10}'.format(\"Fav Strategy: \")+'{:>9}'.format(str(round((Current_Accuracy[\"Fav Strategy\"].sum()/ Current_Accuracy[\"Fav Strategy\"].dropna().shape[0])*100,2))+\"% \\n\"),\n",
    "     '{:10}'.format(\"Home Pick: \")+'{:>10}'.format(str(round((Current_Accuracy[\"Home Pick\"].sum()/ Current_Accuracy[\"Home Pick\"].dropna().shape[0])*100,2))+\"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
