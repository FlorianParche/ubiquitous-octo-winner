{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains the Result Evaluation.\n",
    "\n",
    "#### Notebook 1: Extract-Transform-Load\n",
    "#### Notebook 2: Data Visualization\n",
    "#### Notebook 3: Feature Engineering, Hyperparameter tuning and Modelling\n",
    "#### Notebook 4: Result Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline \n",
    "from scipy import stats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data import - from the Feature Engineering, Hyperparameter tuning and Modelling notebook\n",
    "Results_data = pd.read_csv(r\"C:\\Users\\### LOCAL PATH ###\\Result_evaluation.txt\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accuracy = pd.DataFrame(columns=[\"Tree Prediction\",\"SVM Prediction\",\"NN Prediction\",\"Democracy\",\"Unanimous\",\"Fav Strategy\",\"Pick Home\"])\n",
    "\n",
    "for i in range(Results_data.shape[0]):\n",
    "    for column in Accuracy.columns:\n",
    "        if str(Results_data.loc[i,column]) == \"nan\":\n",
    "            Accuracy.loc[i,column] = np.nan\n",
    "        \n",
    "        else:\n",
    "            Accuracy.loc[i,column] = np.where(Results_data.loc[i,\"Actual\"] == Results_data.loc[i,column],1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy by approach: \\n \\n\" ,\n",
    "      '{:10}'.format(\"Tree: \")+ '{:>13}'.format(str(round((Accuracy[\"Tree Prediction\"].sum() / Accuracy[\"Tree Prediction\"].dropna().shape[0])*100,2))+\"% \\n\"), \n",
    "      '{:10}'.format(\"SVM: \")+'{:>13}'.format(str(round((Accuracy[\"SVM Prediction\"].sum()/ Accuracy[\"SVM Prediction\"].dropna().shape[0])*100,2))+\"% \\n\"),\n",
    "     '{:10}'.format(\"NN: \")+ '{:>13}'.format(str(round((Accuracy[\"NN Prediction\"].sum()/ Accuracy[\"NN Prediction\"].dropna().shape[0])*100,2))+\"% \\n\"),\n",
    "    '{:10}'.format(\"Democracy: \")+'{:>12}'.format(str(round((Accuracy[\"Democracy\"].sum()/ Accuracy[\"Democracy\"].dropna().shape[0])*100,2))+\"% \\n\"),\n",
    "     '{:10}'.format(\"Unanimous: \")+ '{:>12}'.format(str(round((Accuracy[\"Unanimous\"].sum()/ Accuracy[\"Unanimous\"].dropna().shape[0])*100,2))+\"% \\n\"),\n",
    "      '{:10}'.format(\"Fav Strategy: \")+'{:>9}'.format(str(round((Accuracy[\"Fav Strategy\"].sum()/ Accuracy[\"Fav Strategy\"].dropna().shape[0])*100,2))+\"% \\n\"),\n",
    "     '{:10}'.format(\"Pick Home: \")+'{:>10}'.format(str(round((Accuracy[\"Pick Home\"].sum()/ Accuracy[\"Pick Home\"].dropna().shape[0])*100,2))+\"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_measured = pd.DataFrame(columns= Accuracy.columns)\n",
    "accuracy_measured.loc[0] = Accuracy.mean()\n",
    "accuracy_measured = accuracy_measured.transpose().sort_values(0, ascending = False).rename(columns={0:\"Accuracy %\"})\n",
    "accuracy_measured[\"Accuracy %\"] = round(accuracy_measured[\"Accuracy %\"]*100,2).astype(str)\n",
    "accuracy_measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-tests to determine whether the best strategy is in fact better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variably determining the most accurate strategy - model based strategies\n",
    "accuracy_measured = pd.DataFrame(columns= Accuracy.columns)\n",
    "accuracy_measured.loc[0] = Accuracy.mean()\n",
    "#accuracy_measured = accuracy_measured.drop([\"Fav Strategy\",\"Pick Home\"], axis=1)\n",
    "accuracy_measured = accuracy_measured.transpose().sort_values(0, ascending = False)\n",
    "most_accurate = accuracy_measured.index[0]\n",
    "sec_most_accurate = accuracy_measured.index[1]\n",
    "least_accurate = accuracy_measured.index[-1]\n",
    "print(\"Most/Least accurate: \\n \\n\",\"1. \"+ most_accurate +'\\n', \"2. \"+sec_most_accurate +'\\n', \"Last: \"+least_accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs. \n",
    "print(\"The most accurate (model based) strategy is \"+most_accurate+\" with \"+str(round(accuracy_measured.loc[most_accurate,0]*100,2))+\"% accuracy. \\n\")\n",
    "\n",
    "for model_2 in list(Accuracy.columns):\n",
    "    if most_accurate == model_2:\n",
    "        next\n",
    "    else:\n",
    "        var_most = Accuracy[most_accurate].dropna().var(ddof=1)\n",
    "        var_second = Accuracy[model_2].dropna().var(ddof=1)\n",
    "        s = np.sqrt((var_most / Accuracy[most_accurate].dropna().shape[0] + var_second / Accuracy[model_2].dropna().shape[0]))\n",
    "\n",
    "        t = (accuracy_measured.loc[most_accurate,0] - accuracy_measured.loc[model_2,0]) / s\n",
    "        \n",
    "        if model_2 == \"Fav Strategy\":\n",
    "            print()\n",
    "        if t < 1.65:\n",
    "            print(most_accurate + \" is not significantly more accurate than \"+model_2+\". (t-value: \"+str(round(t,3))+\").\")\n",
    "        elif (t >1.65)& (t<1.96):\n",
    "            print(most_accurate + \" is more accurate than \"+model_2 +\" at the 10% confidence level.\"+\" (t-value: \"+str(round(t,3))+\").\")\n",
    "        elif (t >1.96)& (t<2.58):\n",
    "            print(most_accurate + \" is more accurate than \"+model_2 +\" at the 5% confidence level.\"+\" (t-value: \"+str(round(t,3))+\").\")\n",
    "        elif (t>2.58):\n",
    "            print(most_accurate + \" is more accurate than \"+model_2 +\" at the 1% confidence level.\"+\" (t-value: \"+str(round(t,3))+\").\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs. \n",
    "\n",
    "print(\"The NN strategy has \"+str(round(accuracy_measured.loc[\"NN Prediction\",0]*100,2))+\"% accuracy. \\n\")\n",
    "\n",
    "for model_2 in list(Accuracy.columns):\n",
    "    if model_2 in [\"NN Prediction\"] :\n",
    "        next\n",
    "    else:\n",
    "        var_most = Accuracy[\"NN Prediction\"].dropna().var(ddof=1)\n",
    "        var_second = Accuracy[model_2].dropna().var(ddof=1)\n",
    "        s = np.sqrt((var_most / Accuracy[\"NN Prediction\"].dropna().shape[0] + var_second / Accuracy[model_2].dropna().shape[0]))\n",
    "\n",
    "        t = (accuracy_measured.loc[\"NN Prediction\",0] - accuracy_measured.loc[model_2,0]) / s\n",
    "        \n",
    "        if model_2 == \"Fav Strategy\":\n",
    "            print()\n",
    "        if t < 1.65:\n",
    "            print(\"NN strategy\" + \" is not significantly more accurate than \"+model_2+\". (t-value: \"+str(round(t,3))+\").\")\n",
    "        elif (t >1.65)& (t<1.96):\n",
    "            print(\"NN strategy\" + \" is more accurate than \"+model_2 +\" at the 10% confidence level.\"+\" (t-value: \"+str(round(t,3))+\").\")\n",
    "        elif (t >1.96)& (t<2.58):\n",
    "            print(\"NN strategy\" + \" is more accurate than \"+model_2 +\" at the 5% confidence level.\"+\" (t-value: \"+str(round(t,3))+\").\")\n",
    "        elif (t>2.58):\n",
    "            print(\"NN strategy\" + \" is more accurate than \"+model_2 +\" at the 1% confidence level.\"+\" (t-value: \"+str(round(t,3))+\").\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(df_confusion, title, cmap=plt.cm.gray_r):\n",
    "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "    plt.title(title,y = 1.2, size=15)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns)\n",
    "    plt.yticks(tick_marks, df_confusion.index)\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel(df_confusion.index.name, labelpad=15)\n",
    "    plt.xlabel(df_confusion.columns.name, labelpad=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = Results_data[\"Actual\"]\n",
    "y_predicted = Results_data[\"Tree Prediction\"]\n",
    "\n",
    "df_confusion = pd.crosstab(y_actual,y_predicted, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "try:\n",
    "    x = df_confusion[\"D\"].shape[0]\n",
    "except:\n",
    "    df_confusion[\"D\"] = 0\n",
    "    df_confusion = df_confusion[[\"A\",\"D\",\"H\"]]\n",
    "\n",
    "plot_confusion_matrix(df_confusion, \"Confusion Matrix - Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support-Vector-Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = Results_data[\"Actual\"]\n",
    "y_predicted = Results_data[\"SVM Prediction\"]\n",
    "\n",
    "df_confusion = pd.crosstab(y_actual,y_predicted, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "try:\n",
    "    x = df_confusion[\"D\"].shape[0]\n",
    "except:\n",
    "    df_confusion[\"D\"] = 0\n",
    "    df_confusion = df_confusion[[\"A\",\"D\",\"H\"]]\n",
    "\n",
    "plot_confusion_matrix(df_confusion, \"Confusion Matrix - SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = Results_data[\"Actual\"]\n",
    "y_predicted = Results_data[\"NN Prediction\"]\n",
    "\n",
    "df_confusion = pd.crosstab(y_actual,y_predicted, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "try:\n",
    "    x = df_confusion[\"D\"].shape[0]\n",
    "except:\n",
    "    df_confusion[\"D\"] = 0\n",
    "    df_confusion = df_confusion[[\"A\",\"D\",\"H\"]]\n",
    "\n",
    "plot_confusion_matrix(df_confusion, \"Confusion Matrix - NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Democracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = Results_data[\"Actual\"]\n",
    "y_predicted = Results_data[\"Democracy\"]\n",
    "\n",
    "df_confusion = pd.crosstab(y_actual,y_predicted, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "try:\n",
    "    x = df_confusion[\"D\"].shape[0]\n",
    "except:\n",
    "    df_confusion[\"D\"] = 0\n",
    "    df_confusion = df_confusion[[\"A\",\"D\",\"H\"]]\n",
    "\n",
    "plot_confusion_matrix(df_confusion, \"Confusion Matrix - Democracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unanimous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = Results_data[\"Actual\"]\n",
    "y_predicted = Results_data[\"Unanimous\"]\n",
    "\n",
    "df_confusion = pd.crosstab(y_actual,y_predicted, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "try:\n",
    "    x = df_confusion[\"D\"].shape[0]\n",
    "except:\n",
    "    df_confusion[\"D\"] = 0\n",
    "    df_confusion = df_confusion[[\"A\",\"D\",\"H\"]]\n",
    "\n",
    "\n",
    "plot_confusion_matrix(df_confusion, \"Confusion Matrix - Unanimous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fav Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = Results_data[\"Actual\"]\n",
    "y_predicted = Results_data[\"Fav Strategy\"]\n",
    "\n",
    "df_confusion = pd.crosstab(y_actual,y_predicted, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "try:\n",
    "    x = df_confusion[\"D\"].shape[0]\n",
    "except:\n",
    "    df_confusion[\"D\"] = 0\n",
    "    df_confusion = df_confusion[[\"A\",\"D\",\"H\"]]\n",
    "\n",
    "\n",
    "plot_confusion_matrix(df_confusion, \"Confusion Matrix - Favorites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the model is not omptimized, nor designed, to exploit discrepancies between estimated and by odds implied game result probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assuming bets of 1 unit, every game, on the most likely outcome according to the respective model / strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_calc(Dataframe):\n",
    "    Dataframe = Dataframe.dropna().reset_index(drop=True)\n",
    "    Dataframe[\"correct\"] = np.nan\n",
    "    Dataframe[\"Payoff\"] = np.nan\n",
    "    \n",
    "    for i in range(Dataframe.shape[0]):\n",
    "        Dataframe.loc[i,\"correct\"] = np.where(Dataframe.loc[i,\"Actual\"] == Dataframe.iloc[i,1], 1, 0)\n",
    "        if Dataframe.loc[i,\"Actual\"] == \"A\":\n",
    "            Dataframe.loc[i,\"Payoff\"] = np.where(Dataframe.loc[i,\"correct\"] == 1, Dataframe.loc[i,\"Odds A\"] -1, -1)\n",
    "        elif Dataframe.loc[i,\"Actual\"] == \"D\":\n",
    "            Dataframe.loc[i,\"Payoff\"] = np.where(Dataframe.loc[i,\"correct\"] == 1, Dataframe.loc[i,\"Odds D\"] -1, -1)\n",
    "        elif Dataframe.loc[i,\"Actual\"] == \"H\":\n",
    "            Dataframe.loc[i,\"Payoff\"] = np.where(Dataframe.loc[i,\"correct\"] == 1, Dataframe.loc[i,\"Odds H\"] -1, -1)\n",
    "        else:\n",
    "            print(\"ERROR\")\n",
    "    \n",
    "    absolute_return = Dataframe[\"Payoff\"].sum()\n",
    "    ROI = absolute_return / Dataframe.shape[0]\n",
    "    \n",
    "    return absolute_return, ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing absolute and relative returns by strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abs_Tree, ROI_Tree = return_calc(Results_data.loc[:,[\"Actual\",\"Tree Prediction\",\"Odds A\", \"Odds D\",\"Odds H\"]])\n",
    "abs_SVM, ROI_SVM = return_calc(Results_data.loc[:,[\"Actual\",\"SVM Prediction\",\"Odds A\", \"Odds D\",\"Odds H\"]])\n",
    "abs_NN, ROI_NN = return_calc(Results_data.loc[:,[\"Actual\",\"NN Prediction\",\"Odds A\", \"Odds D\",\"Odds H\"]])\n",
    "abs_Democracy, ROI_Democracy = return_calc(Results_data.loc[:,[\"Actual\",\"Democracy\",\"Odds A\", \"Odds D\",\"Odds H\"]])\n",
    "abs_Unanimous, ROI_Unanimous = return_calc(Results_data.loc[:,[\"Actual\",\"Unanimous\",\"Odds A\", \"Odds D\",\"Odds H\"]])\n",
    "abs_Fav, ROI_Fav = return_calc(Results_data.loc[:,[\"Actual\",\"Fav Strategy\",\"Odds A\", \"Odds D\",\"Odds H\"]])\n",
    "abs_HP, ROI_HP = return_calc(Results_data.loc[:,[\"Actual\",\"Pick Home\",\"Odds A\", \"Odds D\",\"Odds H\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Absolute and return on investment by approach: \\n \\n\" ,\n",
    "      '{:>21}'.format(\"Absolute\")+ '{:>9}'.format(\"ROI \\n\"),\n",
    "      '{:10}'.format(\"Tree: \")+ '{:>10}'.format(str(round(abs_Tree,2))) +'{:>11}'.format(str(round(ROI_Tree*100,2))+\"% \\n\"), \n",
    "      '{:10}'.format(\"SVM: \")+'{:>10}'.format(str(round(abs_SVM,2))) +'{:>11}'.format(str(round(ROI_SVM*100,2))+\"% \\n\"),\n",
    "     '{:10}'.format(\"NN: \")+ '{:>10}'.format(str(round(abs_NN,2))) +'{:>11}'.format(str(round(ROI_NN*100,2))+\"% \\n\"),\n",
    "    '{:10}'.format(\"Democracy: \")+'{:>9}'.format(str(round(abs_Democracy,2))) +'{:>11}'.format(str(round(ROI_Democracy*100,2))+\"% \\n\"),\n",
    "     '{:10}'.format(\"Unanimous: \")+ '{:>9}'.format(str(round(abs_Unanimous,2))) +'{:>11}'.format(str(round(ROI_Unanimous*100,2))+\"% \\n\"),\n",
    "      '{:10}'.format(\"Fav Strategy: \")+'{:>15}'.format(str(round(abs_Fav,2)) +'{:>11}'.format(str(round(ROI_Fav*100,2))+\"% \\n\")),\n",
    "      '{:10}'.format(\"Home Pick: \")+'{:>18}'.format(str(round(abs_HP,2)) +'{:>9}'.format(str(round(ROI_HP*100,2))+\"%\"))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Returns = pd.DataFrame(columns = [\"Strategy\",\"Absolute\",\"ROI in %\"])\n",
    "\n",
    "Returns.loc[0] = [\"Tree\",abs_Tree,ROI_Tree*100]\n",
    "Returns.loc[1] = [\"SVM\",abs_SVM,ROI_SVM*100]\n",
    "Returns.loc[2] = [\"NN\",abs_NN,ROI_NN*100]\n",
    "Returns.loc[3] = [\"Democracy\",abs_Democracy,ROI_Democracy*100]\n",
    "Returns.loc[4] = [\"Unanimous\",abs_Unanimous,ROI_Unanimous*100]\n",
    "Returns.loc[5] = [\"Fav Strategy\",abs_Fav,ROI_Fav*100]\n",
    "Returns.loc[6] = [\"Home Pick\",abs_HP,ROI_HP*100]\n",
    "Returns.sort_values([\"ROI in %\",\"Absolute\"], ascending = [False, False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Returns_grp = Returns.groupby(\"Strategy\",as_index=True).sum()\n",
    "Returns_df = pd.DataFrame(columns=[\"Absolute\",\"ROI in %\"])\n",
    "Returns_df.loc[\"NN\"] = Returns_grp.loc[\"NN\",:]\n",
    "Returns_df.loc[\"Tree\"] = Returns_grp.loc[\"Tree\",:]\n",
    "Returns_df.loc[\"SVM\"] = Returns_grp.loc[\"SVM\",:]\n",
    "Returns_df.loc[\"Democracy\"] = Returns_grp.loc[\"Democracy\",:]\n",
    "Returns_df.loc[\"Unanimous\"] = Returns_grp.loc[\"Unanimous\",:]\n",
    "Returns_df.loc[\"Fav Strategy\"] = Returns_grp.loc[\"Fav Strategy\",:]\n",
    "Returns_df.loc[\"Home Pick\"] = Returns_grp.loc[\"Home Pick\",:]\n",
    "Returns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_list = [\"silver\", \"gold\"]\n",
    "\n",
    "df = Returns_df.rename(columns={\"ROI in %\":\"ROI\"})\n",
    "\n",
    "ax = df.plot(kind='barh', figsize =(10,7), color = colors_list, edgecolor='w')\n",
    "\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"Absolute and relative return by strategy\", size = 16)\n",
    "\n",
    "ax.set_xlabel(\"Result\", size = 13)\n",
    "ax.set_ylabel(\"Strategy\", size = 13)\n",
    "ax.set_xlim(-100,100)\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for i in ax.patches:\n",
    "    counter +=1\n",
    "    \n",
    "    if i.get_width() < 0:\n",
    "        if str(i.get_y())[-1:] == \"5\":\n",
    "            ax.text(2,  i.get_y()+0.18, (str(round((i.get_width()),2))), fontsize=11, color='red')\n",
    "        else:\n",
    "            ax.text(2,  i.get_y()+0.18, (str(round((i.get_width()),2))+\"%\"), fontsize=11, color='red')\n",
    "    else:\n",
    "        if str(i.get_y())[-1:] == \"5\":\n",
    "            ax.text(-13,  i.get_y()+0.18, (str(round((i.get_width()),2))), fontsize=11, color='green')\n",
    "        else:\n",
    "            ax.text(-13,  i.get_y()+0.18, (str(round((i.get_width()),2))+\"%\"), fontsize=11, color='green')\n",
    "    \n",
    "ax.invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team-Bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accuracy = Accuracy.rename(columns={\"Tree Prediction\":\"Tree Acc\",\"SVM Prediction\":\"SVM Acc\",\"NN Prediction\":\"NN Acc\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.concat([Results_data[[\"Home Team\",\"Away Team\",\"Tree Prediction\",\"SVM Prediction\",\"NN Prediction\"]], Accuracy[[\"Tree Acc\",\"SVM Acc\",\"NN Acc\"]]], axis=1)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Teams_list = list(set(list(Data[\"Home Team\"])+(list(Data[\"Away Team\"]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data_Team_Bias = pd.DataFrame(columns=[\"Team\",\"Tree for\",\"Tree for correct\",\"Tree against\",\"Tree against correct\",\n",
    "                                      \"SVM for\",\"SVM for correct\",\"SVM against\",\"SVM against correct\",\n",
    "                                      \"NN for\",\"NN for correct\",\"NN against\",\"NN against correct\"])\n",
    "i = 0\n",
    "for Team in Teams_list:\n",
    "    df1 = Data.loc[Data[\"Home Team\"] == Team]\n",
    "    df2 = Data.loc[Data[\"Away Team\"] == Team]\n",
    "    \n",
    "    for model in [\"Tree\",\"SVM\",\"NN\"]:\n",
    "        if model == \"Tree\": \n",
    "            T_H_for = df1.loc[df1[\"Tree Prediction\"]==\"H\"].shape[0]\n",
    "            T_H_against = df1.loc[df1[\"Tree Prediction\"]!=\"H\"].shape[0]\n",
    "            \n",
    "            T_H_for_correct = df1.loc[df1[\"Tree Prediction\"]==\"H\"][\"Tree Acc\"].sum()\n",
    "            T_H_against_correct = df1.loc[df1[\"Tree Prediction\"]!=\"H\"][\"Tree Acc\"].sum()\n",
    "            \n",
    "            ###\n",
    "            T_A_for = df2.loc[df2[\"Tree Prediction\"]==\"A\"].shape[0]\n",
    "            T_A_against = df2.loc[df2[\"Tree Prediction\"]!=\"A\"].shape[0]\n",
    "            \n",
    "            T_A_for_correct = df2.loc[df2[\"Tree Prediction\"]==\"A\"][\"Tree Acc\"].sum()\n",
    "            T_A_against_correct = df2.loc[df2[\"Tree Prediction\"]!=\"A\"][\"Tree Acc\"].sum()\n",
    "            \n",
    "            T_for = T_H_for + T_A_for\n",
    "            T_for_corr = T_H_for_correct + T_A_for_correct\n",
    "            T_against = T_H_against + T_A_against\n",
    "            T_against_corr = T_H_against_correct + T_A_against_correct\n",
    "            \n",
    "        elif model == \"SVM\": \n",
    "            S_H_for = df1.loc[df1[\"SVM Prediction\"]==\"H\"].shape[0]\n",
    "            S_H_against = df1.loc[df1[\"SVM Prediction\"]!=\"H\"].shape[0]\n",
    "            \n",
    "            S_H_for_correct = df1.loc[df1[\"SVM Prediction\"]==\"H\"][\"Tree Acc\"].sum()\n",
    "            S_H_against_correct = df1.loc[df1[\"SVM Prediction\"]!=\"H\"][\"Tree Acc\"].sum()\n",
    "            \n",
    "            ###\n",
    "            S_A_for = df2.loc[df2[\"SVM Prediction\"]==\"A\"].shape[0]\n",
    "            S_A_against = df2.loc[df2[\"SVM Prediction\"]!=\"A\"].shape[0]\n",
    "            \n",
    "            S_A_for_correct = df2.loc[df2[\"SVM Prediction\"]==\"A\"][\"Tree Acc\"].sum()\n",
    "            S_A_against_correct = df2.loc[df2[\"SVM Prediction\"]!=\"A\"][\"Tree Acc\"].sum()\n",
    "            \n",
    "            S_for = S_H_for + S_A_for\n",
    "            S_for_corr = S_H_for_correct + S_A_for_correct\n",
    "            S_against = S_H_against + S_A_against\n",
    "            S_against_corr = S_H_against_correct + S_A_against_correct\n",
    "            \n",
    "        elif model == \"NN\": \n",
    "            N_H_for = df1.loc[df1[\"NN Prediction\"]==\"H\"].shape[0]\n",
    "            N_H_against = df1.loc[df1[\"NN Prediction\"]!=\"H\"].shape[0]\n",
    "            \n",
    "            N_H_for_correct = df1.loc[df1[\"NN Prediction\"]==\"H\"][\"Tree Acc\"].sum()\n",
    "            N_H_against_correct = df1.loc[df1[\"NN Prediction\"]!=\"H\"][\"Tree Acc\"].sum()\n",
    "            \n",
    "            ###\n",
    "            N_A_for = df2.loc[df2[\"NN Prediction\"]==\"A\"].shape[0]\n",
    "            N_A_against = df2.loc[df2[\"NN Prediction\"]!=\"A\"].shape[0]\n",
    "            \n",
    "            N_A_for_correct = df2.loc[df2[\"NN Prediction\"]==\"A\"][\"Tree Acc\"].sum()\n",
    "            N_A_against_correct = df2.loc[df2[\"NN Prediction\"]!=\"A\"][\"Tree Acc\"].sum()\n",
    "            \n",
    "            N_for = N_H_for + N_A_for\n",
    "            N_for_corr = N_H_for_correct + N_A_for_correct\n",
    "            N_against = N_H_against + N_A_against\n",
    "            N_against_corr = N_H_against_correct + N_A_against_correct  \n",
    "    \n",
    "    data = [Team, T_for, T_for_corr, T_against, T_against_corr, S_for, S_for_corr, S_against, S_against_corr,\n",
    "            N_for, N_for_corr, N_against, N_against_corr]\n",
    "    \n",
    "    Data_Team_Bias.loc[i] = data\n",
    "    i += 1\n",
    "    \n",
    "Data_Team_Bias = Data_Team_Bias.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Team_Bias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When picking the Team to win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tree_for_data = Data_Team_Bias.loc[:,[\"Team\",\"Tree for\",\"Tree for correct\"]]\n",
    "Tree_for_data = Tree_for_data.rename(columns={\"Tree for\":\"pred Win\",\"Tree for correct\":\"Correct\"})\n",
    "Tree_for_data[\"%\"] = np.nan\n",
    "for i in range(Tree_for_data.shape[0]):\n",
    "    try:\n",
    "        Tree_for_data.loc[i,\"%\"] = round(Tree_for_data.loc[i,\"Correct\"] / Tree_for_data.loc[i,\"pred Win\"] *100,2)\n",
    "    except:\n",
    "        Tree_for_data.loc[i,\"%\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree_for_data.loc[Tree_for_data[\"pred Win\"] >= 5].dropna().sort_values(\"%\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree_for_data.loc[Tree_for_data[\"pred Win\"] >= 5].dropna().sort_values(\"%\", ascending = False).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When picking against the Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tree_for_data = Data_Team_Bias.loc[:,[\"Team\",\"Tree against\",\"Tree against correct\"]]\n",
    "Tree_for_data = Tree_for_data.rename(columns={\"Tree against\":\"pred not Win\",\"Tree against correct\":\"Correct\"})\n",
    "Tree_for_data[\"%\"] = np.nan\n",
    "for i in range(Tree_for_data.shape[0]):\n",
    "    try:\n",
    "        Tree_for_data.loc[i,\"%\"] = round(Tree_for_data.loc[i,\"Correct\"] / Tree_for_data.loc[i,\"pred not Win\"] *100,2)\n",
    "    except:\n",
    "        Tree_for_data.loc[i,\"%\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree_for_data.loc[Tree_for_data[\"pred not Win\"] >= 5].dropna().sort_values(\"%\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree_for_data.loc[Tree_for_data[\"pred not Win\"] >= 5].dropna().sort_values(\"%\", ascending = False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When picking the Team to win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_for_data = Data_Team_Bias.loc[:,[\"Team\",\"SVM for\",\"SVM for correct\"]]\n",
    "SVM_for_data[\"%\"] = np.nan\n",
    "for i in range(SVM_for_data.shape[0]):\n",
    "    try:\n",
    "        SVM_for_data.loc[i,\"%\"] = round(SVM_for_data.loc[i,\"SVM for correct\"] / SVM_for_data.loc[i,\"SVM for\"] *100,2)\n",
    "    except:\n",
    "        SVM_for_data.loc[i,\"%\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_for_data.loc[SVM_for_data[\"SVM for\"] >= 5].dropna().sort_values(\"%\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_for_data.loc[SVM_for_data[\"SVM for\"] >= 5].dropna().sort_values(\"%\", ascending = False).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When picking against the Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_ag_data = Data_Team_Bias.loc[:,[\"Team\",\"SVM against\",\"SVM against correct\"]]\n",
    "SVM_ag_data[\"%\"] = np.nan\n",
    "for i in range(SVM_ag_data.shape[0]):\n",
    "    try:\n",
    "        SVM_ag_data.loc[i,\"%\"] = round(SVM_ag_data.loc[i,\"SVM against correct\"] / SVM_ag_data.loc[i,\"SVM against\"] *100,2)\n",
    "    except:\n",
    "        SVM_ag_data.loc[i,\"%\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_ag_data.loc[SVM_ag_data[\"SVM against\"] >= 5].dropna().sort_values(\"%\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_ag_data.loc[SVM_ag_data[\"SVM against\"] >= 5].dropna().sort_values(\"%\", ascending = False).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When picking the Team to win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_for_data = Data_Team_Bias.loc[:,[\"Team\",\"NN for\",\"NN for correct\"]]\n",
    "NN_for_data[\"%\"] = np.nan\n",
    "for i in range(NN_for_data.shape[0]):\n",
    "    try:\n",
    "        NN_for_data.loc[i,\"%\"] = round(NN_for_data.loc[i,\"NN for correct\"] / NN_for_data.loc[i,\"NN for\"] *100,2)\n",
    "    except:\n",
    "        NN_for_data.loc[i,\"%\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_for_data.loc[NN_for_data[\"NN for\"] >= 5].dropna().sort_values(\"%\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_for_data.loc[NN_for_data[\"NN for\"] >= 5].dropna().sort_values(\"%\", ascending = False).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When picking against the Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_ag_data = Data_Team_Bias.loc[:,[\"Team\",\"NN against\",\"NN against correct\"]]\n",
    "NN_ag_data[\"%\"] = np.nan\n",
    "for i in range(NN_ag_data.shape[0]):\n",
    "    try:\n",
    "        NN_ag_data.loc[i,\"%\"] = round(NN_ag_data.loc[i,\"NN against correct\"] / NN_ag_data.loc[i,\"NN against\"] *100,2)\n",
    "    except:\n",
    "        NN_ag_data.loc[i,\"%\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_ag_data.loc[NN_ag_data[\"NN against\"] >= 5].dropna().sort_values(\"%\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_ag_data.loc[NN_ag_data[\"NN against\"] >= 5].dropna().sort_values(\"%\", ascending = False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
